<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Video Reconstruction on Notes</title><link>https://HowardHsuuu.github.io/tags/video-reconstruction/</link><description>Recent content in Video Reconstruction on Notes</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 30 Mar 2025 10:51:36 +0800</lastBuildDate><atom:link href="https://HowardHsuuu.github.io/tags/video-reconstruction/index.xml" rel="self" type="application/rss+xml"/><item><title>[LR] Reconstruction of Dynamic Natural Vision from Slow Brain Activity</title><link>https://HowardHsuuu.github.io/posts/lr/mind-animator/</link><pubDate>Sun, 30 Mar 2025 10:51:36 +0800</pubDate><guid>https://HowardHsuuu.github.io/posts/lr/mind-animator/</guid><description>&lt;p>[This review is intended solely for my personal learning]&lt;/p>
&lt;p>Paper Info&lt;/p>
&lt;blockquote>
&lt;p>arXiv: 2405.03280v2&lt;br>
Title: Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural Vision from Slow Brain Activity&lt;br>
Authors: Yizhuo Lu, Changde Du, Chong Wang, Xuanliu Zhu, Liuyun Jiang, Xujin Li, and Huiguang He&lt;/p>&lt;/blockquote>
&lt;h2 id="prior-knowledge">Prior Knowledge&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Brain decoding with fMRI&lt;/strong>: Functional MRI measures brain activity through BOLD (Blood-Oxygen-Level Dependent) signals, which reflect neural activation across 3D voxel grids. Decoding methods aim to map these signals back to perceptual experiences like images or videos.&lt;/li>
&lt;li>&lt;strong>Contrastive learning and CLIP&lt;/strong>: Contrastive loss functions (e.g., InfoNCE) train models to align representations from different modalities. CLIP is a pre-trained vision-language model that embeds text and images into a shared space, useful for representing high-level semantics.&lt;/li>
&lt;li>&lt;strong>VQ-VAE (Vector Quantized Variational Autoencoder)&lt;/strong>: A generative model that discretizes image data into latent tokens, enabling compression and reconstruction. It’s commonly used in diffusion models for reducing the input dimensionality while preserving structure.&lt;/li>
&lt;li>&lt;strong>Transformer and sparse causal attention&lt;/strong>: Transformers model long-range dependencies via attention. Sparse causal attention masks future tokens and restricts attention to a sparse set, ensuring temporal consistency while reducing computational cost—critical for decoding motion from fMRI.&lt;/li>
&lt;li>&lt;strong>Stable Diffusion and U-Net&lt;/strong>: Stable Diffusion is a text-to-image model that generates visuals via iterative denoising in a latent space. Its U-Net backbone processes image latents at multiple resolutions and is commonly inflated for video generation.&lt;/li>
&lt;/ul>
&lt;h2 id="goal">Goal&lt;/h2>
&lt;p>To accurately reconstruct dynamic natural vision (videos) from fMRI recordings by explicitly disentangling and decoding &lt;strong>semantic&lt;/strong>, &lt;strong>structural&lt;/strong>, and &lt;strong>motion&lt;/strong> features — each aligned with distinct neural processes — in order to improve both reconstruction fidelity and neurobiological interpretability.&lt;/p></description></item></channel></rss>