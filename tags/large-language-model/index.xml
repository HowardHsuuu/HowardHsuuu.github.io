<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Large Language Model on Notes</title><link>https://HowardHsuuu.github.io/tags/large-language-model/</link><description>Recent content in Large Language Model on Notes</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 25 Mar 2025 20:12:03 +0800</lastBuildDate><atom:link href="https://HowardHsuuu.github.io/tags/large-language-model/index.xml" rel="self" type="application/rss+xml"/><item><title>[LR] Representation Engineering: Top-Down AI Transparency</title><link>https://HowardHsuuu.github.io/posts/lr/repe/</link><pubDate>Tue, 25 Mar 2025 20:12:03 +0800</pubDate><guid>https://HowardHsuuu.github.io/posts/lr/repe/</guid><description>&lt;p>[This review is intended solely for my personal learning]&lt;/p>
&lt;p>Paper Info&lt;/p>
&lt;blockquote>
&lt;p>arXiv: 2310.01405v4&lt;br>
Title: Representation Engineering: A Top-Down Approach to AI Transparency&lt;br>
Authors: Evan Hubinger, Ajeya Cotra, Buck Shlegeris, Tom Lieberum, Nicholas Joseph, Owain Evans, Nicholas Schiefer, Oliver Zhang, Jan Brauner, Collin Burns, Leo Gao, Ryan Greenblatt&lt;/p>&lt;/blockquote>
&lt;hr>
&lt;h2 id="prior-knowledge">Prior Knowledge&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Interpretability in AI&lt;/strong>: Traditional approaches often focus on low-level features such as neurons or circuits. However, such bottom-up analysis struggles with high-level abstractions like honesty or power-seeking.&lt;/li>
&lt;li>&lt;strong>Representation Learning&lt;/strong>: Neural networks form internal embeddings of concepts during training, enabling powerful generalization and emergent behavior.&lt;/li>
&lt;li>&lt;strong>Mechanistic Interpretability&lt;/strong>: Aims to reverse-engineer model internals to understand reasoning processes, but often lacks tools for directly modifying internal representations to improve safety.&lt;/li>
&lt;li>&lt;strong>Transparency and Alignment&lt;/strong>: Transparency is critical for ensuring model behavior is aligned with human intentions and values, particularly to avoid deceptive alignment and inner misalignment.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="goal">Goal&lt;/h2>
&lt;p>The paper proposes and develops &lt;strong>Representation Engineering (RepE)&lt;/strong>, a new top-down framework for interpretability and model control. Rather than focusing on neurons or circuits, RepE centers on &lt;strong>representations&lt;/strong> of high-level cognitive concepts and aims to understand, detect, and manipulate them. The core goal is to advance transparency methods that are directly useful for improving &lt;strong>AI safety&lt;/strong>, including controlling undesirable behaviors like deception or power-seeking.&lt;/p></description></item><item><title>[LR] LLMs Enhancement via Negative Emotional Stimuli</title><link>https://HowardHsuuu.github.io/posts/lr/negativeprompt/</link><pubDate>Sat, 02 Nov 2024 20:02:13 +0800</pubDate><guid>https://HowardHsuuu.github.io/posts/lr/negativeprompt/</guid><description>&lt;p>[This review is intended solely for my personal learning]&lt;/p>
&lt;p>Paper Info&lt;/p>
&lt;blockquote>
&lt;p>arXiv: 2405.02814&lt;br>
Title: NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli&lt;br>
Authors: Xu Wang, Cheng Li, Yi Chang, Jindong Wang, Yuan Wu&lt;/p>&lt;/blockquote>
&lt;h2 id="prior-knowledge">Prior Knowledge&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Emotion and Cognition in AI&lt;/strong>: Previous research has shown that positive emotional stimuli improve LLM performance, raising the question of whether negative emotional stimuli can also have an impact.&lt;/li>
&lt;li>&lt;strong>Psychological Theories&lt;/strong>: The study draws on Cognitive Dissonance Theory, Social Comparison Theory, and Stress and Coping Theory to design negative emotional prompts that may influence LLM responses.&lt;/li>
&lt;/ul>
&lt;h2 id="goal">Goal&lt;/h2>
&lt;p>To explore whether negative emotional stimuli, integrated into prompts, can enhance the performance of LLMs across various NLP tasks. The study introduces &lt;em>NegativePrompt&lt;/em>, a novel prompting technique that applies negative emotional cues to improve LLM output quality.&lt;/p></description></item><item><title>[LR] Unveiling Theory of Mind in LLMs</title><link>https://HowardHsuuu.github.io/posts/lr/llm-tom/</link><pubDate>Tue, 29 Oct 2024 19:24:43 +0800</pubDate><guid>https://HowardHsuuu.github.io/posts/lr/llm-tom/</guid><description>&lt;p>[This review is intended solely for my personal learning]&lt;/p>
&lt;p>Paper Info&lt;/p>
&lt;blockquote>
&lt;p>arXiv:2309.01660&lt;br>
Title: Unveiling Theory of Mind in Large Language Models: A Parallel to Single Neurons in the Human Brain&lt;br>
Author: Mohsen Jamali and Ziv M. Williams and Jing Cai&lt;/p>&lt;/blockquote>
&lt;h2 id="prior-knowledge">Prior Knowledge&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Theory of Mind (ToM)&lt;/strong>:
A complex cognitive capacity related to our conscious mind and mental state that allows us to infer another&amp;rsquo;s beliefs and perspective. Through ToM, human can create intricate mental representations of other agents and realize that others may have beliefs that&amp;rsquo;s different from our own or the objective reality.&lt;/li>
&lt;li>&lt;strong>True- and False-belief Task&lt;/strong>
&lt;ul>
&lt;li>True-belief task: assesses whether someone understands that some other people&amp;rsquo;s believes is correctly aligned with reality.&lt;/li>
&lt;li>False-belief task: assesses whether someone understands that some other people&amp;rsquo;s believes is &lt;strong>not&lt;/strong> correctly aligned with reality. (ex: belief diverges from reality after a change to the environment that one did not witness.)&lt;/li>
&lt;li>A critical test for ToM is the false belief task.&lt;/li>
&lt;li>Both tasks are evaluated by providing the participant a scenario and asking the participant &amp;ldquo;fact questions&amp;rdquo; and &amp;ldquo;belief questions&amp;rdquo;, which are about the reality and the belief of some character in the scenario respectively.&lt;/li>
&lt;li>These tasks are designed to test if the individual can attribute mental states (including potentially false beliefs) to others in general.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>ToM in the human brain&lt;/strong>&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Human brain imaging studies have provided substantial evidence for the brain network that supports our ToM ability, including the temporalparietal junction, superior temporal sulcus and the dorsal medial prefrontal cortex (dmPFC)&lt;/p></description></item></channel></rss>