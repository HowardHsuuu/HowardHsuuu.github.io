<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Notes</title><link>https://HowardHsuuu.github.io/</link><description>Recent content on Notes</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 09 Feb 2025 20:01:09 +0800</lastBuildDate><atom:link href="https://HowardHsuuu.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>[LR] The Flatland Fallacy</title><link>https://HowardHsuuu.github.io/posts/lr/flatland-fallacy/</link><pubDate>Sun, 09 Feb 2025 20:01:09 +0800</pubDate><guid>https://HowardHsuuu.github.io/posts/lr/flatland-fallacy/</guid><description>&lt;p>[This review is intended solely for my personal learning]&lt;/p>
&lt;h2 id="paper-info">Paper Info&lt;/h2>
&lt;blockquote>
&lt;p>DOI: 10.1111/tops.12404&lt;br>
Title: The Flatland Fallacy: Moving Beyond Low–Dimensional Thinking&lt;br>
Authors: Eshin Jolly, Luke J. Chang&lt;/p>&lt;/blockquote>
&lt;h2 id="prior-knowledge">Prior Knowledge&lt;/h2>
&lt;p>Psychological and cognitive sciences have long relied on simplified, low-dimensional models to explain complex human behavior. While these models provide theoretical clarity and empirical tractability, they often fail to capture the full intricacy of psychological phenomena. The reliance on two-factor or low-dimensional frameworks—such as dual-process theories of cognition—raises concerns about scientific oversimplification.&lt;/p></description></item><item><title>[LR] The Emerging Science of Interacting Minds</title><link>https://HowardHsuuu.github.io/posts/lr/interacting-minds/</link><pubDate>Sat, 08 Feb 2025 18:28:13 +0800</pubDate><guid>https://HowardHsuuu.github.io/posts/lr/interacting-minds/</guid><description>&lt;p>[This review is intended solely for my personal learning]&lt;/p>
&lt;h2 id="paper-info">Paper Info&lt;/h2>
&lt;blockquote>
&lt;p>DOI: 10.1177/17456916231200177&lt;br>
Title: The Emerging Science of Interacting Minds&lt;br>
Authors: Thalia Wheatley, Mark A. Thornton, Arjen Stolk, and Luke J. Chang&lt;/p>&lt;/blockquote>
&lt;h2 id="prior-knowledge">Prior Knowledge&lt;/h2>
&lt;p>Historically, psychology has focused on the individual, often isolating cognitive processes within a single brain. However, social interaction is fundamental to human cognition, development, and collective behavior. While past research in social psychology and neuroscience has explored elements of interaction, methodological constraints have limited empirical studies that directly analyze interaction dynamics.&lt;/p></description></item><item><title>[LR] TD-MPC2</title><link>https://HowardHsuuu.github.io/posts/lr/tdmpc2/</link><pubDate>Thu, 21 Nov 2024 20:32:33 +0800</pubDate><guid>https://HowardHsuuu.github.io/posts/lr/tdmpc2/</guid><description>&lt;p>[This review is intended solely for my personal learning]&lt;/p>
&lt;h2 id="paper-info">Paper Info&lt;/h2>
&lt;blockquote>
&lt;p>Title: TD-MPC2: Scalable, Robust World Models for Continuous Control&lt;br>
Authors: Nicklas Hansen, Hao Su, Xiaolong Wang&lt;br>
Conference: ICLR 2024&lt;br>
arXiv: 2310.16828&lt;/p>&lt;/blockquote>
&lt;h2 id="prior-knowledge">Prior Knowledge&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Model-Based Reinforcement Learning (MBRL)&lt;/strong>: Uses an internal model of the environment to plan and optimize actions rather than learning policies from direct interaction alone.&lt;/li>
&lt;li>&lt;strong>Temporal Difference Learning&lt;/strong>: A method in RL that estimates the value function iteratively using bootstrapped learning.&lt;/li>
&lt;li>&lt;strong>Model Predictive Control (MPC)&lt;/strong>: An optimization framework for selecting actions over a finite horizon using a learned world model.&lt;/li>
&lt;li>&lt;strong>TD-MPC&lt;/strong>: A prior algorithm that performs local trajectory optimization in the latent space of an implicit world model but lacks scalability and robustness.&lt;/li>
&lt;/ul>
&lt;h2 id="goal">Goal&lt;/h2>
&lt;p>The authors propose &lt;strong>TD-MPC2&lt;/strong>, an extension of the TD-MPC framework, designed to scale reinforcement learning to large, uncurated datasets and generalize across multiple continuous control tasks. The key aims are:&lt;/p></description></item><item><title>[LR] LLMs Enhancement via Negative Emotional Stimuli</title><link>https://HowardHsuuu.github.io/posts/lr/negativeprompt/</link><pubDate>Sat, 02 Nov 2024 20:02:13 +0800</pubDate><guid>https://HowardHsuuu.github.io/posts/lr/negativeprompt/</guid><description>&lt;p>[This review is intended solely for my personal learning]&lt;/p>
&lt;h2 id="paper-info">Paper Info&lt;/h2>
&lt;blockquote>
&lt;p>arXiv: 2405.02814&lt;br>
Title: NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli&lt;br>
Authors: Xu Wang, Cheng Li, Yi Chang, Jindong Wang, Yuan Wu&lt;/p>&lt;/blockquote>
&lt;h2 id="prior-knowledge">Prior Knowledge&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Emotion and Cognition in AI&lt;/strong>: Previous research has shown that positive emotional stimuli improve LLM performance, raising the question of whether negative emotional stimuli can also have an impact.&lt;/li>
&lt;li>&lt;strong>Psychological Theories&lt;/strong>: The study draws on Cognitive Dissonance Theory, Social Comparison Theory, and Stress and Coping Theory to design negative emotional prompts that may influence LLM responses.&lt;/li>
&lt;/ul>
&lt;h2 id="goal">Goal&lt;/h2>
&lt;p>To explore whether negative emotional stimuli, integrated into prompts, can enhance the performance of LLMs across various NLP tasks. The study introduces &lt;em>NegativePrompt&lt;/em>, a novel prompting technique that applies negative emotional cues to improve LLM output quality.&lt;/p></description></item><item><title>[LR] Unveiling Theory of Mind in LLMs</title><link>https://HowardHsuuu.github.io/posts/lr/llm-tom/</link><pubDate>Tue, 29 Oct 2024 19:24:43 +0800</pubDate><guid>https://HowardHsuuu.github.io/posts/lr/llm-tom/</guid><description>&lt;p>[This review is intended solely for my personal learning]&lt;/p>
&lt;p>Paper Info&lt;/p>
&lt;blockquote>
&lt;p>arXiv:2309.01660&lt;br>
Title: Unveiling Theory of Mind in Large Language Models: A Parallel to Single Neurons in the Human Brain&lt;br>
Author: Mohsen Jamali and Ziv M. Williams and Jing Cai&lt;/p>&lt;/blockquote>
&lt;h2 id="prior-knowledge">Prior Knowledge&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Theory of Mind (ToM)&lt;/strong>:
A complex cognitive capacity related to our conscious mind and mental state that allows us to infer another&amp;rsquo;s beliefs and perspective. Through ToM, human can create intricate mental representations of other agents and realize that others may have beliefs that&amp;rsquo;s different from our own or the objective reality.&lt;/li>
&lt;li>&lt;strong>True- and False-belief Task&lt;/strong>
&lt;ul>
&lt;li>True-belief task: assesses whether someone understands that some other people&amp;rsquo;s believes is correctly aligned with reality.&lt;/li>
&lt;li>False-belief task: assesses whether someone understands that some other people&amp;rsquo;s believes is &lt;strong>not&lt;/strong> correctly aligned with reality. (ex: belief diverges from reality after a change to the environment that one did not witness.)&lt;/li>
&lt;li>A critical test for ToM is the false belief task.&lt;/li>
&lt;li>Both tasks are evaluated by providing the participant a scenario and asking the participant &amp;ldquo;fact questions&amp;rdquo; and &amp;ldquo;belief questions&amp;rdquo;, which are about the reality and the belief of some character in the scenario respectively.&lt;/li>
&lt;li>These tasks are designed to test if the individual can attribute mental states (including potentially false beliefs) to others in general.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>ToM in the human brain&lt;/strong>&lt;/li>
&lt;/ul>
&lt;blockquote>
&lt;p>Human brain imaging studies have provided substantial evidence for the brain network that supports our ToM ability, including the temporalparietal junction, superior temporal sulcus and the dorsal medial prefrontal cortex (dmPFC)&lt;/p></description></item><item><title>[LR] Binocular Vision SSVEP BCI for Dual-Frequency Modulation</title><link>https://HowardHsuuu.github.io/posts/lr/ssvep-bci/</link><pubDate>Sat, 12 Oct 2024 20:32:51 +0800</pubDate><guid>https://HowardHsuuu.github.io/posts/lr/ssvep-bci/</guid><description>&lt;p>[This review is intended solely for my personal learning]&lt;/p>
&lt;p>Paper Info&lt;/p>
&lt;blockquote>
&lt;p>DOI: 10.1109/TBME.2022.3212192&lt;br>
Title: A Binocular Vision SSVEP Brain–Computer Interface Paradigm for Dual-Frequency Modulation&lt;br>
Authors: Yike Sun, Liyan Liang, Jingnan Sun, Xiaogang Chen, Runfa Tian, Yuanfang Chen, Lijian Zhang, and Xiaorong Gao&lt;/p>&lt;/blockquote>
&lt;h2 id="prior-knowledge">Prior Knowledge&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>SSVEP and BCIs:&lt;/strong> Steady-State Visual Evoked Potentials (SSVEPs) are brain responses elicited by periodic visual stimuli. Their robustness and high signal-to-noise ratio make them a cornerstone in non-invasive BCI research&lt;/li>
&lt;li>&lt;strong>Dual-Frequency Stimulation:&lt;/strong> Traditional dual-frequency paradigms, such as the checkerboard arrangement, allow the encoding of more targets but are hampered by intermodulation artifacts, which can compromise signal quality.&lt;/li>
&lt;li>&lt;strong>Binocular Vision Approach:&lt;/strong> By using circularly polarized light to deliver different frequencies to each eye, the binocular vision paradigm minimizes interference from intermodulation harmonics, thereby enhancing signal fidelity.&lt;/li>
&lt;/ul>
&lt;h2 id="goal">Goal&lt;/h2>
&lt;p>To design and evaluate a novel dual-frequency SSVEP paradigm based on binocular vision that suppresses intermodulation harmonics and enhances overall BCI performance, particularly in training-free applications.&lt;/p></description></item><item><title>[LR] Clonemator: Spatiotemporal Clones in VR</title><link>https://HowardHsuuu.github.io/posts/lr/clonemator/</link><pubDate>Fri, 27 Sep 2024 19:56:26 +0800</pubDate><guid>https://HowardHsuuu.github.io/posts/lr/clonemator/</guid><description>&lt;p>[This review is intended solely for my personal learning]&lt;/p>
&lt;p>[The VR project, &lt;strong>ChronoClones&lt;/strong>, in OpenHCI 2024 was based on this paper]&lt;/p>
&lt;p>Paper Info&lt;/p>
&lt;blockquote>
&lt;p>arXiv: 2311.04427&lt;br>
Title: Clonemator: Composing Spatiotemporal Clones to Create Interactive Automators in Virtual Reality&lt;br>
Authors: Yi-Shuo Lin, Ching-Yi Tsai, Lung-Pan Cheng&lt;/p>&lt;/blockquote>
&lt;h2 id="prior-knowledge">Prior Knowledge&lt;/h2>
&lt;ul>
&lt;li>&lt;strong>Interaction Techniques in VR&lt;/strong>: Traditional approaches (e.g., Go-Go, portals) focus on specific tasks or single-avatar enhancements. While helpful, these designs rarely offer a broad, integrative way to handle diverse, complex interactions without pre-coded solutions.&lt;/li>
&lt;li>&lt;strong>Programming by Demonstration (PbD)&lt;/strong>: Non-VR automation tools like Sikuli or Ringer let users capture and replay interactions to automate tasks. However, applying PbD to immersive VR—where physical presence, body movement, and 3D spatial context matter—is underexplored.&lt;/li>
&lt;/ul>
&lt;h2 id="goal">Goal&lt;/h2>
&lt;p>Clonemator aims to let users create and collaborate with virtual “clones” of themselves to accomplish complex or repetitive tasks, all without manual scripting. By allowing clones to be configured across space (e.g., different locations, scales) and time (e.g., static pose, synchronous motion, or replayed actions), the system seeks to empower users to construct flexible, intuitive “automators” for tasks ranging from object manipulation to cooperative assemblies.&lt;/p></description></item><item><title>[LR] Quixer: A Quantum Transformer Model</title><link>https://HowardHsuuu.github.io/posts/lr/quixer/</link><pubDate>Fri, 27 Sep 2024 19:56:26 +0800</pubDate><guid>https://HowardHsuuu.github.io/posts/lr/quixer/</guid><description>&lt;p>[This review is intended solely for my personal learning]&lt;/p>
&lt;p>[&lt;strong>Qiix&lt;/strong> at Qiskit Hackathon Taiwan 2024 was inspired by this paper]&lt;/p>
&lt;p>Paper Info&lt;/p>
&lt;blockquote>
&lt;p>arXiv: 2406.04305&lt;br>
Title: Quixer: A Quantum Transformer Model&lt;br>
Authors: Nikhil Khatri, Gabriel Matos, Luuk Coopmans, Stephen Clark (Quantinuum)&lt;/p>&lt;/blockquote>
&lt;h2 id="prior-knowledge">Prior Knowledge&lt;/h2>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>Linear Combination of Unitaries (LCU)&lt;/strong>&lt;br>
The LCU technique allows the combination of multiple unitary operators into one overall operator using additional control qubits and postselection. Given unitaries $U_0, U_1, \dots, U_{n-1}$ with complex coefficients $b_0, b_1, \dots, b_{n-1}$, one can effectively prepare a block-encoded operator
$$
M = \sum_{j=0}^{n-1} b_j U_j.
$$&lt;/p></description></item><item><title>OTSea Staking - lost 26k</title><link>https://HowardHsuuu.github.io/posts/defi-analysis/otsea-staking/</link><pubDate>Wed, 25 Sep 2024 23:06:01 +0800</pubDate><guid>https://HowardHsuuu.github.io/posts/defi-analysis/otsea-staking/</guid><description>&lt;h2 id="what-happened">What happened&lt;/h2>
&lt;p>Staking contract, OTSeaStaking, hacked and lost 26k. Hacker exploited the contract&amp;rsquo;s logic flaw, which allowed him/her to call &amp;ldquo;withdraw&amp;rdquo; many times and got a lot more tokens than he staked.&lt;/p>
&lt;h2 id="the-problem">The problem&lt;/h2>
&lt;p>In &lt;a href="https://etherscan.io/address/0x5da151b95657e788076d04d56234bd93e409cb09#code%23F21%23L396">line 396 of OTSeaStaking.sol&lt;/a>, you can see that &lt;code>deposit.amount&lt;/code> is not handled properly (not decreased); therefore, one can deposit once and withdraw multiple times.&lt;/p>
&lt;h2 id="poc">PoC&lt;/h2>
&lt;p>The PoC of this incident I wrote can be found &lt;a href="https://github.com/HowardHsuuu/DeFiHackLabs?tab=readme-ov-file#20240913-OTSeaStaking---Logic-Flaw">here&lt;/a>&lt;/p>
&lt;h2 id="reference">Reference&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://app.blocksec.com/explorer/tx/eth/0x90b4fcf583444d44efb8625e6f253cfcb786d2f4eda7198bdab67a54108cd5f4">transaction hash&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://nickfranklin.site/2024/09/13/otsea-staking-hacked/">https://nickfranklin.site/2024/09/13/otsea-staking-hacked/&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>[LR] Interfacing with Lucid Dreams</title><link>https://HowardHsuuu.github.io/posts/lr/lucid-interface/</link><pubDate>Wed, 25 Sep 2024 20:33:52 +0800</pubDate><guid>https://HowardHsuuu.github.io/posts/lr/lucid-interface/</guid><description>&lt;p>[This review is intended solely for my personal learning]&lt;/p>
&lt;p>Paper Info - LuciEntry&lt;/p>
&lt;blockquote>
&lt;p>DOI: 10.1145/3613905.3649123&lt;br>
Title: LuciEntry: A Modular Lab-based Lucid Dreaming Induction Prototype&lt;br>
Authors: Po-Yao (Cosmos) Wang, Nathaniel Lee Yung Xiang, Rohit Rajesh, Antony Smith Loose, Nathan Semertzidis, and Florian ‘Floyd’ Mueller&lt;/p>&lt;/blockquote>
&lt;p>Paper Info - DreamCeption&lt;/p>
&lt;blockquote>
&lt;p>DOI: 10.1145/3613905.3649121&lt;br>
Title: DreamCeption: Towards Understanding the Design of Targeted Lucid Dream Mediation&lt;br>
Authors: Po-Yao (Cosmos) Wang, Rohit Rajesh, Antony Smith Loose, Nathaniel Lee Yung Xiang, Nathalie Overdevest, Nathan Semertzidis, and Florian ‘Floyd’ Mueller&lt;/p></description></item><item><title>Chih-Hao (Howard) Hsu</title><link>https://HowardHsuuu.github.io/aboutme/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://HowardHsuuu.github.io/aboutme/</guid><description>&lt;h2 id="projects">Projects&lt;/h2>
&lt;table style="width: 100%; border: 1px solid #ccc; border-collapse: collapse;">
&lt;!-- First project row -->
&lt;tr style="border-bottom: 1px solid #ccc;">
&lt;!-- Left cell: image/thumbnail -->
&lt;td style="vertical-align: top; padding: 16px; width: 300px;">
&lt;a href="https://example.com" target="_blank">
&lt;img
src="https://HowardHsuuu.github.io/images/makentu.png"
alt="Automatic Smart Storage Assistant"
style="width: 300px; height: 250px; object-fit: cover; border: none;"
/>
&lt;/a>
&lt;/td>
&lt;td style="vertical-align: top; padding: 16px;">
&lt;strong>Automatic Smart Storage Assistant&lt;/strong>&lt;br>&lt;br>
&lt;strong>Award:&lt;/strong> &lt;span style="font-size: 16px;">MakeNTU NXP Enterprise Prize 2nd Place&lt;/span>&lt;br>
&lt;strong>Description:&lt;/strong> &lt;br>
&lt;/td>
&lt;/tr>
&lt;!-- Second project row -->
&lt;tr>
&lt;td style="vertical-align: top; padding: 16px;">
&lt;a href="https://example.com" target="_blank">
&lt;img
src="https://HowardHsuuu.github.io/images/chronoclones.png"
alt="ChronoClones"
style="width: 300px; height: 250px; object-fit: cover; border: none;"
/>
&lt;/a>
&lt;/td>
&lt;td style="vertical-align: top; padding: 16px;">
&lt;strong>ChronoClones&lt;/strong>&lt;br>&lt;br>
&lt;strong>Award:&lt;/strong> &lt;span style="font-size: 16px;">OpenHCI Best Technical Award&lt;/span>&lt;br>
&lt;strong>Description:&lt;/strong> &lt;br>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="vertical-align: top; padding: 16px;">
&lt;a href="https://example.com" target="_blank">
&lt;img
src="https://HowardHsuuu.github.io/images/qiix.png"
alt="Qiix"
style="width: 300px; height: 220px; object-fit: cover; border: none;"
/>
&lt;/a>
&lt;/td>
&lt;td style="vertical-align: top; padding: 16px;">
&lt;strong>Qiix – &lt;span style="font-size: 16px;">improved Quantum image encoding for quantum machine learning with information mixer&lt;/span>&lt;/strong>&lt;br>&lt;br>
&lt;strong>Award:&lt;/strong> &lt;span style="font-size: 16px;">Qiskit Hackathon Taiwan Enterprize Prize&lt;/span>&lt;br>
&lt;strong>Description:&lt;/strong> &lt;br>
[&lt;a href="https://docs.google.com/presentation/d/1NdBtXrslF3GRsoMWn8kVqhPO0MtfVos8R_ykd5IWcos/edit?usp=sharing">slide&lt;/a>]
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td style="vertical-align: top; padding: 16px;">
&lt;a href="https://example.com" target="_blank">
&lt;img
src="https://HowardHsuuu.github.io/images/ctci.png"
alt="AI-Assisted Critical Care Simulation System"
style="width: 300px; height: 220px; object-fit: cover; border: none;"
/>
&lt;/a>
&lt;/td>
&lt;td style="vertical-align: top; padding: 16px;">
&lt;strong>AI-Assisted Critical Care Simulation System&lt;/strong>&lt;br>&lt;br>
&lt;strong>Award:&lt;/strong> &lt;span style="font-size: 16px;">CTCI Foundation AI
Innovation Competition 1st Place&lt;/span>&lt;br>
&lt;strong>Description:&lt;/strong> &lt;br>
[&lt;a href="https://www.ctci.org.tw/media/10233/1ai%E8%BC%94%E5%8A%A9%E9%87%8D%E7%97%87%E7%97%85%E6%88%BF%E6%A8%A1%E6%93%AC%E7%B3%BB%E7%B5%B1.pdf">poster&lt;/a>]
&lt;/td>
&lt;/tr>
&lt;/table></description></item></channel></rss>