<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[LR] TD-MPC2 | Notes</title>
<meta name=keywords content="Reinforcement Learning,Robotics"><meta name=description content="[This review is intended solely for my personal learning]
Paper Info

Title: TD-MPC2: Scalable, Robust World Models for Continuous Control
Authors: Nicklas Hansen, Hao Su, Xiaolong Wang
Conference: ICLR 2024
arXiv: 2310.16828
Prior Knowledge

Model-Based Reinforcement Learning (MBRL): Uses an internal model of the environment to plan and optimize actions rather than learning policies from direct interaction alone.
Temporal Difference Learning: A method in RL that estimates the value function iteratively using bootstrapped learning.
Model Predictive Control (MPC): An optimization framework for selecting actions over a finite horizon using a learned world model.
TD-MPC: A prior algorithm that performs local trajectory optimization in the latent space of an implicit world model but lacks scalability and robustness.

Goal
The authors propose TD-MPC2, an extension of the TD-MPC framework, designed to scale reinforcement learning to large, uncurated datasets and generalize across multiple continuous control tasks. The key aims are:"><meta name=author content><link rel=canonical href=https://HowardHsuuu.github.io/posts/lr/tdmpc2/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://HowardHsuuu.github.io/images/icon.png><link rel=icon type=image/png sizes=16x16 href=https://HowardHsuuu.github.io/images/icon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://HowardHsuuu.github.io/images/icon-32x32.png><link rel=apple-touch-icon href=https://HowardHsuuu.github.io/images/icon.png><link rel=mask-icon href=https://HowardHsuuu.github.io/images/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://HowardHsuuu.github.io/posts/lr/tdmpc2/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-...some_hash... crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-...some_hash... crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-...some_hash... crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="[LR] TD-MPC2"><meta property="og:description" content="[This review is intended solely for my personal learning]
Paper Info

Title: TD-MPC2: Scalable, Robust World Models for Continuous Control
Authors: Nicklas Hansen, Hao Su, Xiaolong Wang
Conference: ICLR 2024
arXiv: 2310.16828
Prior Knowledge

Model-Based Reinforcement Learning (MBRL): Uses an internal model of the environment to plan and optimize actions rather than learning policies from direct interaction alone.
Temporal Difference Learning: A method in RL that estimates the value function iteratively using bootstrapped learning.
Model Predictive Control (MPC): An optimization framework for selecting actions over a finite horizon using a learned world model.
TD-MPC: A prior algorithm that performs local trajectory optimization in the latent space of an implicit world model but lacks scalability and robustness.

Goal
The authors propose TD-MPC2, an extension of the TD-MPC framework, designed to scale reinforcement learning to large, uncurated datasets and generalize across multiple continuous control tasks. The key aims are:"><meta property="og:type" content="article"><meta property="og:url" content="https://HowardHsuuu.github.io/posts/lr/tdmpc2/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-11-21T20:32:33+08:00"><meta property="article:modified_time" content="2024-11-21T20:32:33+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="[LR] TD-MPC2"><meta name=twitter:description content="[This review is intended solely for my personal learning]
Paper Info

Title: TD-MPC2: Scalable, Robust World Models for Continuous Control
Authors: Nicklas Hansen, Hao Su, Xiaolong Wang
Conference: ICLR 2024
arXiv: 2310.16828
Prior Knowledge

Model-Based Reinforcement Learning (MBRL): Uses an internal model of the environment to plan and optimize actions rather than learning policies from direct interaction alone.
Temporal Difference Learning: A method in RL that estimates the value function iteratively using bootstrapped learning.
Model Predictive Control (MPC): An optimization framework for selecting actions over a finite horizon using a learned world model.
TD-MPC: A prior algorithm that performs local trajectory optimization in the latent space of an implicit world model but lacks scalability and robustness.

Goal
The authors propose TD-MPC2, an extension of the TD-MPC framework, designed to scale reinforcement learning to large, uncurated datasets and generalize across multiple continuous control tasks. The key aims are:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Topics","item":"https://HowardHsuuu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Literature Review","item":"https://HowardHsuuu.github.io/posts/lr/"},{"@type":"ListItem","position":3,"name":"[LR] TD-MPC2","item":"https://HowardHsuuu.github.io/posts/lr/tdmpc2/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[LR] TD-MPC2","name":"[LR] TD-MPC2","description":"[This review is intended solely for my personal learning]\nPaper Info Title: TD-MPC2: Scalable, Robust World Models for Continuous Control\nAuthors: Nicklas Hansen, Hao Su, Xiaolong Wang\nConference: ICLR 2024\narXiv: 2310.16828\nPrior Knowledge Model-Based Reinforcement Learning (MBRL): Uses an internal model of the environment to plan and optimize actions rather than learning policies from direct interaction alone. Temporal Difference Learning: A method in RL that estimates the value function iteratively using bootstrapped learning. Model Predictive Control (MPC): An optimization framework for selecting actions over a finite horizon using a learned world model. TD-MPC: A prior algorithm that performs local trajectory optimization in the latent space of an implicit world model but lacks scalability and robustness. Goal The authors propose TD-MPC2, an extension of the TD-MPC framework, designed to scale reinforcement learning to large, uncurated datasets and generalize across multiple continuous control tasks. The key aims are:\n","keywords":["Reinforcement Learning","Robotics"],"articleBody":"[This review is intended solely for my personal learning]\nPaper Info Title: TD-MPC2: Scalable, Robust World Models for Continuous Control\nAuthors: Nicklas Hansen, Hao Su, Xiaolong Wang\nConference: ICLR 2024\narXiv: 2310.16828\nPrior Knowledge Model-Based Reinforcement Learning (MBRL): Uses an internal model of the environment to plan and optimize actions rather than learning policies from direct interaction alone. Temporal Difference Learning: A method in RL that estimates the value function iteratively using bootstrapped learning. Model Predictive Control (MPC): An optimization framework for selecting actions over a finite horizon using a learned world model. TD-MPC: A prior algorithm that performs local trajectory optimization in the latent space of an implicit world model but lacks scalability and robustness. Goal The authors propose TD-MPC2, an extension of the TD-MPC framework, designed to scale reinforcement learning to large, uncurated datasets and generalize across multiple continuous control tasks. The key aims are:\nAlgorithmic robustness: Achieve strong performance across diverse tasks using a single set of hyperparameters. Scalability: Ensure performance improves with increasing model and data sizes. Generalization: Train a single agent to solve multiple continuous control problems across different action spaces and embodiments. Method TD-MPC2 introduces several improvements over its predecessor:\n1. Learning an Implicit World Model TD-MPC2 employs a decoder-free world model, where observations are mapped into a latent representation $z$ and optimized using a combination of:\nJoint-embedding prediction: Encourages representations to be predictive without reconstructing observations. Reward prediction: Estimates task-specific rewards directly from the latent space. Temporal Difference (TD) Learning: Uses a learned value function to bootstrap future returns beyond the planning horizon. The key components of the world model include:\nEncoder $z = h(s, e)$: Maps observations to latent states. Latent dynamics $z’ = d(z, a, e)$: Predicts future latent states given an action. Reward function $r̂ = R(z, a, e)$: Predicts expected rewards. Terminal value function $q̂ = Q(z, a, e)$: Estimates long-term return. Policy prior $â = p(z, e)$: Provides an initial estimate of the best action. 2. Model Predictive Control with a Policy Prior TD-MPC2 integrates MPC with a learned policy prior to optimize actions iteratively. The optimization process:\nSamples candidate action sequences from a Gaussian distribution. Evaluates expected returns using the learned world model. Bootstraps values beyond the planning horizon using the terminal value function. Refines action distributions over multiple iterations before execution. 3. Training Generalist TD-MPC2 Agents To achieve generalization across diverse tasks, TD-MPC2 introduces:\nLearnable task embeddings: A vector representation conditioned on each task, allowing the model to generalize across multiple environments. Action masking: Handles varying action spaces by zero-padding and masking invalid dimensions. Results TD-MPC2 is evaluated across 104 continuous control tasks spanning four major domains:\nDMControl (39 tasks) - Locomotion and manipulation challenges. Meta-World (50 tasks) - Robotic manipulation tasks. ManiSkill2 (5 tasks) - Realistic robotic skill learning. MyoSuite (10 tasks) - Physiologically accurate musculoskeletal control. Key Findings: State-of-the-Art Performance: Outperforms SAC, DreamerV3, and TD-MPC on all four benchmarks. Scalability: Performance improves as model and dataset sizes increase. Generalization: A 317M parameter agent is successfully trained to perform 80 diverse tasks with a single set of hyperparameters. Efficiency: Achieves superior sample efficiency and robustness compared to prior methods. Conclusion TD-MPC2 represents a major step toward scalable and robust model-based reinforcement learning. By integrating latent trajectory optimization, multi-task generalization, and implicit world modeling, the framework achieves strong performance across a wide range of continuous control tasks. The key contributions include:\nA decoder-free world model that facilitates scalable learning. A policy prior-enhanced planning approach for robust action selection. A single hyperparameter set enabling broad generalization. Scalability insights, demonstrating that larger models consistently improve performance. TD-MPC2 highlights the potential of generalist world models in reinforcement learning. Limitations High computational cost: Training large world models requires significant GPU resources. Thoughts Future work could explore leveraging pre-trained models for zero-shot learning.\nReferences The paper: https://arxiv.org/abs/2310.16828 This note was written with the assistance of Generative AI and is based on the content and results presented in the original paper. ","wordCount":"659","inLanguage":"en-us","datePublished":"2024-11-21T20:32:33+08:00","dateModified":"2024-11-21T20:32:33+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://HowardHsuuu.github.io/posts/lr/tdmpc2/"},"publisher":{"@type":"Organization","name":"Notes","logo":{"@type":"ImageObject","url":"https://HowardHsuuu.github.io/images/icon.png"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://HowardHsuuu.github.io/ accesskey=h title="Notes (Alt + H)"><img src=https://HowardHsuuu.github.io/images/icon.png alt aria-label=logo height=35>Notes</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://HowardHsuuu.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://HowardHsuuu.github.io/posts/ title=Topics><span>Topics</span></a></li><li><a href=https://HowardHsuuu.github.io/tags title=Tags><span>Tags</span></a></li><li><a href=https://HowardHsuuu.github.io/aboutme/ title="About Me"><span>About Me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://HowardHsuuu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://HowardHsuuu.github.io/posts/>Topics</a>&nbsp;»&nbsp;<a href=https://HowardHsuuu.github.io/posts/lr/>Literature Review</a></div><h1 class="post-title entry-hint-parent">[LR] TD-MPC2</h1><div class=post-meta><span title='2024-11-21 20:32:33 +0800 +0800'>November 21, 2024</span>&nbsp;·&nbsp;4 min</div></header><div class=post-content><p>[This review is intended solely for my personal learning]</p><h2 id=paper-info>Paper Info<a hidden class=anchor aria-hidden=true href=#paper-info>#</a></h2><blockquote><p>Title: TD-MPC2: Scalable, Robust World Models for Continuous Control<br>Authors: Nicklas Hansen, Hao Su, Xiaolong Wang<br>Conference: ICLR 2024<br>arXiv: 2310.16828</p></blockquote><h2 id=prior-knowledge>Prior Knowledge<a hidden class=anchor aria-hidden=true href=#prior-knowledge>#</a></h2><ul><li><strong>Model-Based Reinforcement Learning (MBRL)</strong>: Uses an internal model of the environment to plan and optimize actions rather than learning policies from direct interaction alone.</li><li><strong>Temporal Difference Learning</strong>: A method in RL that estimates the value function iteratively using bootstrapped learning.</li><li><strong>Model Predictive Control (MPC)</strong>: An optimization framework for selecting actions over a finite horizon using a learned world model.</li><li><strong>TD-MPC</strong>: A prior algorithm that performs local trajectory optimization in the latent space of an implicit world model but lacks scalability and robustness.</li></ul><h2 id=goal>Goal<a hidden class=anchor aria-hidden=true href=#goal>#</a></h2><p>The authors propose <strong>TD-MPC2</strong>, an extension of the TD-MPC framework, designed to scale reinforcement learning to large, uncurated datasets and generalize across multiple continuous control tasks. The key aims are:</p><ol><li><strong>Algorithmic robustness</strong>: Achieve strong performance across diverse tasks using a single set of hyperparameters.</li><li><strong>Scalability</strong>: Ensure performance improves with increasing model and data sizes.</li><li><strong>Generalization</strong>: Train a single agent to solve multiple continuous control problems across different action spaces and embodiments.</li></ol><h2 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h2><p>TD-MPC2 introduces several improvements over its predecessor:</p><h3 id=1-learning-an-implicit-world-model>1. <strong>Learning an Implicit World Model</strong><a hidden class=anchor aria-hidden=true href=#1-learning-an-implicit-world-model>#</a></h3><p>TD-MPC2 employs a <strong>decoder-free world model</strong>, where observations are mapped into a <strong>latent representation</strong> $z$ and optimized using a combination of:</p><ul><li><strong>Joint-embedding prediction</strong>: Encourages representations to be predictive without reconstructing observations.</li><li><strong>Reward prediction</strong>: Estimates task-specific rewards directly from the latent space.</li><li><strong>Temporal Difference (TD) Learning</strong>: Uses a learned value function to bootstrap future returns beyond the planning horizon.</li></ul><p>The key components of the world model include:</p><ul><li><strong>Encoder</strong> $z = h(s, e)$: Maps observations to latent states.</li><li><strong>Latent dynamics</strong> $z&rsquo; = d(z, a, e)$: Predicts future latent states given an action.</li><li><strong>Reward function</strong> $r̂ = R(z, a, e)$: Predicts expected rewards.</li><li><strong>Terminal value function</strong> $q̂ = Q(z, a, e)$: Estimates long-term return.</li><li><strong>Policy prior</strong> $â = p(z, e)$: Provides an initial estimate of the best action.</li></ul><h3 id=2-model-predictive-control-with-a-policy-prior>2. <strong>Model Predictive Control with a Policy Prior</strong><a hidden class=anchor aria-hidden=true href=#2-model-predictive-control-with-a-policy-prior>#</a></h3><p>TD-MPC2 integrates <strong>MPC</strong> with a learned policy prior to optimize actions iteratively. The optimization process:</p><ul><li><strong>Samples candidate action sequences</strong> from a Gaussian distribution.</li><li><strong>Evaluates expected returns</strong> using the learned world model.</li><li><strong>Bootstraps values</strong> beyond the planning horizon using the terminal value function.</li><li><strong>Refines action distributions</strong> over multiple iterations before execution.</li></ul><h3 id=3-training-generalist-td-mpc2-agents>3. <strong>Training Generalist TD-MPC2 Agents</strong><a hidden class=anchor aria-hidden=true href=#3-training-generalist-td-mpc2-agents>#</a></h3><p>To achieve generalization across diverse tasks, TD-MPC2 introduces:</p><ul><li><strong>Learnable task embeddings</strong>: A vector representation conditioned on each task, allowing the model to generalize across multiple environments.</li><li><strong>Action masking</strong>: Handles varying action spaces by zero-padding and masking invalid dimensions.</li></ul><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p>TD-MPC2 is evaluated across <strong>104 continuous control tasks</strong> spanning four major domains:</p><ol><li><strong>DMControl (39 tasks)</strong> - Locomotion and manipulation challenges.</li><li><strong>Meta-World (50 tasks)</strong> - Robotic manipulation tasks.</li><li><strong>ManiSkill2 (5 tasks)</strong> - Realistic robotic skill learning.</li><li><strong>MyoSuite (10 tasks)</strong> - Physiologically accurate musculoskeletal control.</li></ol><h3 id=key-findings>Key Findings:<a hidden class=anchor aria-hidden=true href=#key-findings>#</a></h3><ul><li><strong>State-of-the-Art Performance</strong>: Outperforms <strong>SAC</strong>, <strong>DreamerV3</strong>, and <strong>TD-MPC</strong> on all four benchmarks.</li><li><strong>Scalability</strong>: Performance improves as model and dataset sizes increase.</li><li><strong>Generalization</strong>: A <strong>317M parameter agent</strong> is successfully trained to perform 80 diverse tasks with a <strong>single set of hyperparameters</strong>.</li><li><strong>Efficiency</strong>: Achieves superior sample efficiency and robustness compared to prior methods.</li></ul><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>TD-MPC2 represents a major step toward <strong>scalable and robust model-based reinforcement learning</strong>. By integrating <strong>latent trajectory optimization</strong>, <strong>multi-task generalization</strong>, and <strong>implicit world modeling</strong>, the framework achieves strong performance across a wide range of continuous control tasks. The key contributions include:</p><ul><li>A <strong>decoder-free</strong> world model that facilitates scalable learning.</li><li>A <strong>policy prior-enhanced planning</strong> approach for robust action selection.</li><li>A <strong>single hyperparameter set</strong> enabling broad generalization.</li><li><strong>Scalability insights</strong>, demonstrating that larger models consistently improve performance.
TD-MPC2 highlights the potential of <strong>generalist world models</strong> in reinforcement learning.</li></ul><h2 id=limitations>Limitations<a hidden class=anchor aria-hidden=true href=#limitations>#</a></h2><ol><li><strong>High computational cost</strong>: Training large world models requires significant GPU resources.</li></ol><h2 id=thoughts>Thoughts<a hidden class=anchor aria-hidden=true href=#thoughts>#</a></h2><p>Future work could explore leveraging pre-trained models for zero-shot learning.</p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li>The paper: <a href=https://arxiv.org/abs/2310.16828>https://arxiv.org/abs/2310.16828</a></li><li>This note was written with the assistance of Generative AI and is based on the content and results presented in the original paper.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://HowardHsuuu.github.io/tags/reinforcement-learning/>Reinforcement Learning</a></li><li><a href=https://HowardHsuuu.github.io/tags/robotics/>Robotics</a></li></ul><nav class=paginav><a class=prev href=https://HowardHsuuu.github.io/posts/lr/interacting-minds/><span class=title>«</span><br><span>[LR] The Emerging Science of Interacting Minds</span>
</a><a class=next href=https://HowardHsuuu.github.io/posts/lr/negativeprompt/><span class=title>»</span><br><span>[LR] LLMs Enhancement via Negative Emotional Stimuli</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://HowardHsuuu.github.io/>Notes</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script type=text/javascript src=/js/canvas-nest.js count=80 color=102,255,178 opacity=1></script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>