<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[LR] Steering Language Models With Activation Engineering | Notes</title>
<meta name=keywords content="Large Language Models,Activation Engineering"><meta name=description content="[This review is intended solely for my personal learning]
Paper Info

arXiv: 2308.10248
Title: Steering Language Models With Activation Engineering
Authors: Alexander Matt Turner, David Udell, Nantas Nardelli, Sam Ringer, Tom McGrath, Eric Michaud, Mantas Mazeika
Prior Knowledge

Steering Language Models: Traditionally achieved through techniques such as prompt engineering, fine-tuning, and reinforcement learning from human feedback (RLHF), which often require substantial computational resources and specialized datasets.
Internal Activation Manipulation: Exploring internal activations of neural networks can offer fine-grained control over their outputs without the cost associated with retraining or extensive model tuning.

Goal
The paper aims to introduce and validate a lightweight inference-time technique, termed Activation Addition (ActAdd), for steering the output of large language models (LLMs). This method targets latent capabilities of LLMs, such as controlled sentiment expression or reduced output toxicity, without the need for additional training."><meta name=author content><link rel=canonical href=https://HowardHsuuu.github.io/posts/lr/actadd/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://HowardHsuuu.github.io/images/icon.png><link rel=icon type=image/png sizes=16x16 href=https://HowardHsuuu.github.io/images/icon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://HowardHsuuu.github.io/images/icon-32x32.png><link rel=apple-touch-icon href=https://HowardHsuuu.github.io/images/icon.png><link rel=mask-icon href=https://HowardHsuuu.github.io/images/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://HowardHsuuu.github.io/posts/lr/actadd/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-...some_hash... crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-...some_hash... crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-...some_hash... crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="[LR] Steering Language Models With Activation Engineering"><meta property="og:description" content="[This review is intended solely for my personal learning]
Paper Info

arXiv: 2308.10248
Title: Steering Language Models With Activation Engineering
Authors: Alexander Matt Turner, David Udell, Nantas Nardelli, Sam Ringer, Tom McGrath, Eric Michaud, Mantas Mazeika
Prior Knowledge

Steering Language Models: Traditionally achieved through techniques such as prompt engineering, fine-tuning, and reinforcement learning from human feedback (RLHF), which often require substantial computational resources and specialized datasets.
Internal Activation Manipulation: Exploring internal activations of neural networks can offer fine-grained control over their outputs without the cost associated with retraining or extensive model tuning.

Goal
The paper aims to introduce and validate a lightweight inference-time technique, termed Activation Addition (ActAdd), for steering the output of large language models (LLMs). This method targets latent capabilities of LLMs, such as controlled sentiment expression or reduced output toxicity, without the need for additional training."><meta property="og:type" content="article"><meta property="og:url" content="https://HowardHsuuu.github.io/posts/lr/actadd/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-26T20:12:03+08:00"><meta property="article:modified_time" content="2025-03-26T20:12:03+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="[LR] Steering Language Models With Activation Engineering"><meta name=twitter:description content="[This review is intended solely for my personal learning]
Paper Info

arXiv: 2308.10248
Title: Steering Language Models With Activation Engineering
Authors: Alexander Matt Turner, David Udell, Nantas Nardelli, Sam Ringer, Tom McGrath, Eric Michaud, Mantas Mazeika
Prior Knowledge

Steering Language Models: Traditionally achieved through techniques such as prompt engineering, fine-tuning, and reinforcement learning from human feedback (RLHF), which often require substantial computational resources and specialized datasets.
Internal Activation Manipulation: Exploring internal activations of neural networks can offer fine-grained control over their outputs without the cost associated with retraining or extensive model tuning.

Goal
The paper aims to introduce and validate a lightweight inference-time technique, termed Activation Addition (ActAdd), for steering the output of large language models (LLMs). This method targets latent capabilities of LLMs, such as controlled sentiment expression or reduced output toxicity, without the need for additional training."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Topics","item":"https://HowardHsuuu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Literature Review","item":"https://HowardHsuuu.github.io/posts/lr/"},{"@type":"ListItem","position":3,"name":"[LR] Steering Language Models With Activation Engineering","item":"https://HowardHsuuu.github.io/posts/lr/actadd/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[LR] Steering Language Models With Activation Engineering","name":"[LR] Steering Language Models With Activation Engineering","description":"[This review is intended solely for my personal learning]\nPaper Info\narXiv: 2308.10248\nTitle: Steering Language Models With Activation Engineering\nAuthors: Alexander Matt Turner, David Udell, Nantas Nardelli, Sam Ringer, Tom McGrath, Eric Michaud, Mantas Mazeika\nPrior Knowledge Steering Language Models: Traditionally achieved through techniques such as prompt engineering, fine-tuning, and reinforcement learning from human feedback (RLHF), which often require substantial computational resources and specialized datasets. Internal Activation Manipulation: Exploring internal activations of neural networks can offer fine-grained control over their outputs without the cost associated with retraining or extensive model tuning. Goal The paper aims to introduce and validate a lightweight inference-time technique, termed Activation Addition (ActAdd), for steering the output of large language models (LLMs). This method targets latent capabilities of LLMs, such as controlled sentiment expression or reduced output toxicity, without the need for additional training.\n","keywords":["Large Language Models","Activation Engineering"],"articleBody":"[This review is intended solely for my personal learning]\nPaper Info\narXiv: 2308.10248\nTitle: Steering Language Models With Activation Engineering\nAuthors: Alexander Matt Turner, David Udell, Nantas Nardelli, Sam Ringer, Tom McGrath, Eric Michaud, Mantas Mazeika\nPrior Knowledge Steering Language Models: Traditionally achieved through techniques such as prompt engineering, fine-tuning, and reinforcement learning from human feedback (RLHF), which often require substantial computational resources and specialized datasets. Internal Activation Manipulation: Exploring internal activations of neural networks can offer fine-grained control over their outputs without the cost associated with retraining or extensive model tuning. Goal The paper aims to introduce and validate a lightweight inference-time technique, termed Activation Addition (ActAdd), for steering the output of large language models (LLMs). This method targets latent capabilities of LLMs, such as controlled sentiment expression or reduced output toxicity, without the need for additional training.\nMethod The proposed Activation Addition method comprises the following steps:\nContrast Pair Selection: Identify pairs of prompts with opposite desired attributes (e.g., positive vs. negative sentiment). Activation Extraction: Run each prompt pair through the LLM and record their internal activations. Steering Vector Computation: Compute a steering vector by calculating the activation difference between the two contrasting prompts. Inference-Time Activation Modification: At inference, add this steering vector to the internal activations of the LLM to steer the model’s output toward the desired attribute. By directly manipulating the model’s internal states, ActAdd provides a precise mechanism for controlling output attributes during inference.\nResults Sentiment Control: ActAdd effectively steered LLM outputs toward positive or negative sentiment, achieving precise control over generated text sentiment. Detoxification Performance: Significantly reduced generation of toxic outputs, surpassing or matching the performance of established detoxification methods across various models (e.g., LLaMA-3, OPT). Preservation of General Performance: ActAdd modifications did not negatively impact model performance on unrelated tasks, indicating targeted and selective intervention. Limitations Dependence on Contrast Pair Quality: ActAdd requires carefully chosen contrastive prompts; ineffective selection could limit its applicability or performance. Generalizability Concerns: Effectiveness across different models and broader tasks beyond those tested remains an area requiring further exploration. Interpretability Challenges: While the technique is practically effective, understanding the specific internal mechanisms and impacts of activation manipulations remains complex. Future work may focus on exploring the applicability and robustness of activation engineering.\nReference The paper: https://arxiv.org/abs/2308.10248 This note was written with the assistance of Generative AI and is based on the content and results presented in the original paper. ","wordCount":"400","inLanguage":"en-us","datePublished":"2025-03-26T20:12:03+08:00","dateModified":"2025-03-26T20:12:03+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://HowardHsuuu.github.io/posts/lr/actadd/"},"publisher":{"@type":"Organization","name":"Notes","logo":{"@type":"ImageObject","url":"https://HowardHsuuu.github.io/images/icon.png"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://HowardHsuuu.github.io/ accesskey=h title="Notes (Alt + H)"><img src=https://HowardHsuuu.github.io/images/icon.png alt aria-label=logo height=35>Notes</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://HowardHsuuu.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://HowardHsuuu.github.io/posts/ title=Topics><span>Topics</span></a></li><li><a href=https://HowardHsuuu.github.io/tags title=Tags><span>Tags</span></a></li><li><a href=https://HowardHsuuu.github.io/aboutme/ title="About Me"><span>About Me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://HowardHsuuu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://HowardHsuuu.github.io/posts/>Topics</a>&nbsp;»&nbsp;<a href=https://HowardHsuuu.github.io/posts/lr/>Literature Review</a></div><h1 class="post-title entry-hint-parent">[LR] Steering Language Models With Activation Engineering</h1><div class=post-meta><span title='2025-03-26 20:12:03 +0800 +0800'>March 26, 2025</span>&nbsp;·&nbsp;2 min</div></header><div class=post-content><p>[This review is intended solely for my personal learning]</p><p>Paper Info</p><blockquote><p>arXiv: 2308.10248<br>Title: Steering Language Models With Activation Engineering<br>Authors: Alexander Matt Turner, David Udell, Nantas Nardelli, Sam Ringer, Tom McGrath, Eric Michaud, Mantas Mazeika</p></blockquote><h2 id=prior-knowledge>Prior Knowledge<a hidden class=anchor aria-hidden=true href=#prior-knowledge>#</a></h2><ul><li><strong>Steering Language Models</strong>: Traditionally achieved through techniques such as prompt engineering, fine-tuning, and reinforcement learning from human feedback (RLHF), which often require substantial computational resources and specialized datasets.</li><li><strong>Internal Activation Manipulation</strong>: Exploring internal activations of neural networks can offer fine-grained control over their outputs without the cost associated with retraining or extensive model tuning.</li></ul><h2 id=goal>Goal<a hidden class=anchor aria-hidden=true href=#goal>#</a></h2><p>The paper aims to introduce and validate a lightweight inference-time technique, termed <strong>Activation Addition (ActAdd)</strong>, for steering the output of large language models (LLMs). This method targets latent capabilities of LLMs, such as controlled sentiment expression or reduced output toxicity, without the need for additional training.</p><h2 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h2><p>The proposed Activation Addition method comprises the following steps:</p><ol><li><strong>Contrast Pair Selection</strong>: Identify pairs of prompts with opposite desired attributes (e.g., positive vs. negative sentiment).</li><li><strong>Activation Extraction</strong>: Run each prompt pair through the LLM and record their internal activations.</li><li><strong>Steering Vector Computation</strong>: Compute a steering vector by calculating the activation difference between the two contrasting prompts.</li><li><strong>Inference-Time Activation Modification</strong>: At inference, add this steering vector to the internal activations of the LLM to steer the model’s output toward the desired attribute.</li></ol><p>By directly manipulating the model&rsquo;s internal states, ActAdd provides a precise mechanism for controlling output attributes during inference.</p><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><ul><li><strong>Sentiment Control</strong>: ActAdd effectively steered LLM outputs toward positive or negative sentiment, achieving precise control over generated text sentiment.</li><li><strong>Detoxification Performance</strong>: Significantly reduced generation of toxic outputs, surpassing or matching the performance of established detoxification methods across various models (e.g., LLaMA-3, OPT).</li><li><strong>Preservation of General Performance</strong>: ActAdd modifications did not negatively impact model performance on unrelated tasks, indicating targeted and selective intervention.</li></ul><h2 id=limitations>Limitations<a hidden class=anchor aria-hidden=true href=#limitations>#</a></h2><ul><li><strong>Dependence on Contrast Pair Quality</strong>: ActAdd requires carefully chosen contrastive prompts; ineffective selection could limit its applicability or performance.</li><li><strong>Generalizability Concerns</strong>: Effectiveness across different models and broader tasks beyond those tested remains an area requiring further exploration.</li><li><strong>Interpretability Challenges</strong>: While the technique is practically effective, understanding the specific internal mechanisms and impacts of activation manipulations remains complex.</li></ul><p>Future work may focus on exploring the applicability and robustness of activation engineering.</p><hr><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><ul><li>The paper: <a href=https://arxiv.org/abs/2308.10248>https://arxiv.org/abs/2308.10248</a></li><li>This note was written with the assistance of Generative AI and is based on the content and results presented in the original paper.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://HowardHsuuu.github.io/tags/large-language-models/>Large Language Models</a></li><li><a href=https://HowardHsuuu.github.io/tags/activation-engineering/>Activation Engineering</a></li></ul><nav class=paginav><a class=prev href=https://HowardHsuuu.github.io/posts/lr/mind-animator/><span class=title>«</span><br><span>[LR] Reconstruction of Dynamic Natural Vision from Slow Brain Activity</span>
</a><a class=next href=https://HowardHsuuu.github.io/posts/lr/repe/><span class=title>»</span><br><span>[LR] Representation Engineering: Top-Down AI Transparency</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://HowardHsuuu.github.io/>Notes</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script type=text/javascript src=/js/canvas-nest.js count=80 color=102,255,178 opacity=1></script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>