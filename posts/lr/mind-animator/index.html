<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[LR] Reconstruction of Dynamic Natural Vision from Slow Brain Activity | Notes</title>
<meta name=keywords content="brain decoding,fMRI,video reconstruction,diffusion,ICLR 2025"><meta name=description content="[This review is intended solely for my personal learning]
Paper Info

arXiv: 2405.03280v2
Title: Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural Vision from Slow Brain Activity
Authors: Yizhuo Lu, Changde Du, Chong Wang, Xuanliu Zhu, Liuyun Jiang, Xujin Li, and Huiguang He
Prior Knowledge

Brain decoding with fMRI: Functional MRI measures brain activity through BOLD (Blood-Oxygen-Level Dependent) signals, which reflect neural activation across 3D voxel grids. Decoding methods aim to map these signals back to perceptual experiences like images or videos.
Contrastive learning and CLIP: Contrastive loss functions (e.g., InfoNCE) train models to align representations from different modalities. CLIP is a pre-trained vision-language model that embeds text and images into a shared space, useful for representing high-level semantics.
VQ-VAE (Vector Quantized Variational Autoencoder): A generative model that discretizes image data into latent tokens, enabling compression and reconstruction. It’s commonly used in diffusion models for reducing the input dimensionality while preserving structure.
Transformer and sparse causal attention: Transformers model long-range dependencies via attention. Sparse causal attention masks future tokens and restricts attention to a sparse set, ensuring temporal consistency while reducing computational cost—critical for decoding motion from fMRI.
Stable Diffusion and U-Net: Stable Diffusion is a text-to-image model that generates visuals via iterative denoising in a latent space. Its U-Net backbone processes image latents at multiple resolutions and is commonly inflated for video generation.

Goal
To accurately reconstruct dynamic natural vision (videos) from fMRI recordings by explicitly disentangling and decoding semantic, structural, and motion features — each aligned with distinct neural processes — in order to improve both reconstruction fidelity and neurobiological interpretability."><meta name=author content><link rel=canonical href=https://HowardHsuuu.github.io/posts/lr/mind-animator/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://HowardHsuuu.github.io/images/icon.png><link rel=icon type=image/png sizes=16x16 href=https://HowardHsuuu.github.io/images/icon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://HowardHsuuu.github.io/images/icon-32x32.png><link rel=apple-touch-icon href=https://HowardHsuuu.github.io/images/icon.png><link rel=mask-icon href=https://HowardHsuuu.github.io/images/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://HowardHsuuu.github.io/posts/lr/mind-animator/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-...some_hash... crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-...some_hash... crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-...some_hash... crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="[LR] Reconstruction of Dynamic Natural Vision from Slow Brain Activity"><meta property="og:description" content="[This review is intended solely for my personal learning]
Paper Info

arXiv: 2405.03280v2
Title: Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural Vision from Slow Brain Activity
Authors: Yizhuo Lu, Changde Du, Chong Wang, Xuanliu Zhu, Liuyun Jiang, Xujin Li, and Huiguang He
Prior Knowledge

Brain decoding with fMRI: Functional MRI measures brain activity through BOLD (Blood-Oxygen-Level Dependent) signals, which reflect neural activation across 3D voxel grids. Decoding methods aim to map these signals back to perceptual experiences like images or videos.
Contrastive learning and CLIP: Contrastive loss functions (e.g., InfoNCE) train models to align representations from different modalities. CLIP is a pre-trained vision-language model that embeds text and images into a shared space, useful for representing high-level semantics.
VQ-VAE (Vector Quantized Variational Autoencoder): A generative model that discretizes image data into latent tokens, enabling compression and reconstruction. It’s commonly used in diffusion models for reducing the input dimensionality while preserving structure.
Transformer and sparse causal attention: Transformers model long-range dependencies via attention. Sparse causal attention masks future tokens and restricts attention to a sparse set, ensuring temporal consistency while reducing computational cost—critical for decoding motion from fMRI.
Stable Diffusion and U-Net: Stable Diffusion is a text-to-image model that generates visuals via iterative denoising in a latent space. Its U-Net backbone processes image latents at multiple resolutions and is commonly inflated for video generation.

Goal
To accurately reconstruct dynamic natural vision (videos) from fMRI recordings by explicitly disentangling and decoding semantic, structural, and motion features — each aligned with distinct neural processes — in order to improve both reconstruction fidelity and neurobiological interpretability."><meta property="og:type" content="article"><meta property="og:url" content="https://HowardHsuuu.github.io/posts/lr/mind-animator/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-03-30T10:51:36+08:00"><meta property="article:modified_time" content="2025-03-30T10:51:36+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="[LR] Reconstruction of Dynamic Natural Vision from Slow Brain Activity"><meta name=twitter:description content="[This review is intended solely for my personal learning]
Paper Info

arXiv: 2405.03280v2
Title: Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural Vision from Slow Brain Activity
Authors: Yizhuo Lu, Changde Du, Chong Wang, Xuanliu Zhu, Liuyun Jiang, Xujin Li, and Huiguang He
Prior Knowledge

Brain decoding with fMRI: Functional MRI measures brain activity through BOLD (Blood-Oxygen-Level Dependent) signals, which reflect neural activation across 3D voxel grids. Decoding methods aim to map these signals back to perceptual experiences like images or videos.
Contrastive learning and CLIP: Contrastive loss functions (e.g., InfoNCE) train models to align representations from different modalities. CLIP is a pre-trained vision-language model that embeds text and images into a shared space, useful for representing high-level semantics.
VQ-VAE (Vector Quantized Variational Autoencoder): A generative model that discretizes image data into latent tokens, enabling compression and reconstruction. It’s commonly used in diffusion models for reducing the input dimensionality while preserving structure.
Transformer and sparse causal attention: Transformers model long-range dependencies via attention. Sparse causal attention masks future tokens and restricts attention to a sparse set, ensuring temporal consistency while reducing computational cost—critical for decoding motion from fMRI.
Stable Diffusion and U-Net: Stable Diffusion is a text-to-image model that generates visuals via iterative denoising in a latent space. Its U-Net backbone processes image latents at multiple resolutions and is commonly inflated for video generation.

Goal
To accurately reconstruct dynamic natural vision (videos) from fMRI recordings by explicitly disentangling and decoding semantic, structural, and motion features — each aligned with distinct neural processes — in order to improve both reconstruction fidelity and neurobiological interpretability."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Topics","item":"https://HowardHsuuu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Literature Review","item":"https://HowardHsuuu.github.io/posts/lr/"},{"@type":"ListItem","position":3,"name":"[LR] Reconstruction of Dynamic Natural Vision from Slow Brain Activity","item":"https://HowardHsuuu.github.io/posts/lr/mind-animator/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[LR] Reconstruction of Dynamic Natural Vision from Slow Brain Activity","name":"[LR] Reconstruction of Dynamic Natural Vision from Slow Brain Activity","description":"[This review is intended solely for my personal learning]\nPaper Info\narXiv: 2405.03280v2\nTitle: Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural Vision from Slow Brain Activity\nAuthors: Yizhuo Lu, Changde Du, Chong Wang, Xuanliu Zhu, Liuyun Jiang, Xujin Li, and Huiguang He\nPrior Knowledge Brain decoding with fMRI: Functional MRI measures brain activity through BOLD (Blood-Oxygen-Level Dependent) signals, which reflect neural activation across 3D voxel grids. Decoding methods aim to map these signals back to perceptual experiences like images or videos. Contrastive learning and CLIP: Contrastive loss functions (e.g., InfoNCE) train models to align representations from different modalities. CLIP is a pre-trained vision-language model that embeds text and images into a shared space, useful for representing high-level semantics. VQ-VAE (Vector Quantized Variational Autoencoder): A generative model that discretizes image data into latent tokens, enabling compression and reconstruction. It’s commonly used in diffusion models for reducing the input dimensionality while preserving structure. Transformer and sparse causal attention: Transformers model long-range dependencies via attention. Sparse causal attention masks future tokens and restricts attention to a sparse set, ensuring temporal consistency while reducing computational cost—critical for decoding motion from fMRI. Stable Diffusion and U-Net: Stable Diffusion is a text-to-image model that generates visuals via iterative denoising in a latent space. Its U-Net backbone processes image latents at multiple resolutions and is commonly inflated for video generation. Goal To accurately reconstruct dynamic natural vision (videos) from fMRI recordings by explicitly disentangling and decoding semantic, structural, and motion features — each aligned with distinct neural processes — in order to improve both reconstruction fidelity and neurobiological interpretability.\n","keywords":["brain decoding","fMRI","video reconstruction","diffusion","ICLR 2025"],"articleBody":"[This review is intended solely for my personal learning]\nPaper Info\narXiv: 2405.03280v2\nTitle: Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural Vision from Slow Brain Activity\nAuthors: Yizhuo Lu, Changde Du, Chong Wang, Xuanliu Zhu, Liuyun Jiang, Xujin Li, and Huiguang He\nPrior Knowledge Brain decoding with fMRI: Functional MRI measures brain activity through BOLD (Blood-Oxygen-Level Dependent) signals, which reflect neural activation across 3D voxel grids. Decoding methods aim to map these signals back to perceptual experiences like images or videos. Contrastive learning and CLIP: Contrastive loss functions (e.g., InfoNCE) train models to align representations from different modalities. CLIP is a pre-trained vision-language model that embeds text and images into a shared space, useful for representing high-level semantics. VQ-VAE (Vector Quantized Variational Autoencoder): A generative model that discretizes image data into latent tokens, enabling compression and reconstruction. It’s commonly used in diffusion models for reducing the input dimensionality while preserving structure. Transformer and sparse causal attention: Transformers model long-range dependencies via attention. Sparse causal attention masks future tokens and restricts attention to a sparse set, ensuring temporal consistency while reducing computational cost—critical for decoding motion from fMRI. Stable Diffusion and U-Net: Stable Diffusion is a text-to-image model that generates visuals via iterative denoising in a latent space. Its U-Net backbone processes image latents at multiple resolutions and is commonly inflated for video generation. Goal To accurately reconstruct dynamic natural vision (videos) from fMRI recordings by explicitly disentangling and decoding semantic, structural, and motion features — each aligned with distinct neural processes — in order to improve both reconstruction fidelity and neurobiological interpretability.\nMethod The proposed system, Mind-Animator, adopts a two-stage pipeline:\nStage 1: fMRI-to-Feature Decoding This stage maps the brain signal $x \\in \\mathbb{R}^n$ (fMRI voxels) into three disentangled representations:\nSemantic Decoder\nProjects fMRI into the CLIP (ViT-B/32) latent space using a tri-modal contrastive learning setup: $$ L_{\\mathrm{semantic}} = \\alpha \\cdot L_{\\mathrm{InfoNCE}}(f, t) + (1 - \\alpha) \\cdot L_{\\mathrm{InfoNCE}}(f, v) $$\nwhere $f$, $t$, $v$ are embeddings from fMRI, text, and video, respectively.\nA 3-layer MLP then maps $f$ to the text-conditioning format $c$ used by Stable Diffusion. Structure Decoder\nUses VQ-VAE encoding of the first frame to represent low-level visual structure. A 2-layer MLP is trained with MSE loss: $$ L_{\\mathrm{structure}} = \\frac{1}{B} \\sum_{i=1}^{B} \\left| D_{\\mathrm{structure}}(f_i) - \\Phi(v_{i,1}) \\right|_2^2 $$\nMotion Decoder: Consistency Motion Generator (CMG)\nA Transformer with sparse causal attention decodes inter-frame motion from latent video tokens. Frame embeddings are predicted autoregressively using fMRI as conditioning input: $L_{\\mathrm{motion}}$ = mean squared error between decoded and ground truth motion tokens Cross-attention ensures that spatial details in fMRI directly affect motion prediction. Stage 2: Feature-to-Video Generation Instead of training a video generation model, the authors inflate a text-to-image Stable Diffusion model into a per-frame generator. Reconstructed features are passed frame-by-frame into the U-Net (SD) and then decoded by VQ-VAE to reconstruct the video clip. Results Evaluated on CC2017, HCP, and Algonauts2021 datasets.\nQuantitative Results (CC2017) Metric Mind-Animator Best Baseline (Mind-Video) SSIM 0.321 0.177 PSNR 9.22 8.87 Hue-PCC 0.786 0.768 CLIP-PCC 0.425 0.409 EPE (↓) 5.42 6.12 VIFI 0.608 0.593 Retrieval accuracy on CC2017 (Top-10, Large Set): 2.39% vs. 1.28% (Mind-Video). Maintains robustness even on extended datasets, demonstrating generalization. Ablation Study Model Variant SSIM VIFI EPE ↓ Full Model 0.319 0.604 5.57 w/o Semantic Decoder 0.097 0.523 8.72 w/o Structure Decoder 0.184 0.555 7.68 w/o Motion Decoder 0.136 0.585 6.37 Removing any one of the decoders leads to a noticeable drop in quality, validating the model’s modular design.\nMotion Validity A shuffle test confirms that motion information arises from fMRI, not model bias. EPE is a more sensitive motion metric than CLIP-PCC in this setting. Interpretability Voxel-wise maps show: High-level cortex (HVC) encodes semantics. V1 and MT jointly encode motion. Ventral cortex for structure. This aligns well with neuroscience literature and supports the brain-inspired design of the decoders.\nLimitations Low temporal resolution of fMRI limits fine-grained motion fidelity. Static structure assumption (only the first frame is used). Potential inductive bias from frozen pretrained components. Reference The paper: https://arxiv.org/abs/2405.03280v2 This note was written with the assistance of Generative AI and is based on the content and results presented in the original paper. ","wordCount":"692","inLanguage":"en-us","datePublished":"2025-03-30T10:51:36+08:00","dateModified":"2025-03-30T10:51:36+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://HowardHsuuu.github.io/posts/lr/mind-animator/"},"publisher":{"@type":"Organization","name":"Notes","logo":{"@type":"ImageObject","url":"https://HowardHsuuu.github.io/images/icon.png"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://HowardHsuuu.github.io/ accesskey=h title="Notes (Alt + H)"><img src=https://HowardHsuuu.github.io/images/icon.png alt aria-label=logo height=35>Notes</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://HowardHsuuu.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://HowardHsuuu.github.io/posts/ title=Topics><span>Topics</span></a></li><li><a href=https://HowardHsuuu.github.io/tags title=Tags><span>Tags</span></a></li><li><a href=https://HowardHsuuu.github.io/aboutme/ title="About Me"><span>About Me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://HowardHsuuu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://HowardHsuuu.github.io/posts/>Topics</a>&nbsp;»&nbsp;<a href=https://HowardHsuuu.github.io/posts/lr/>Literature Review</a></div><h1 class="post-title entry-hint-parent">[LR] Reconstruction of Dynamic Natural Vision from Slow Brain Activity</h1><div class=post-meta><span title='2025-03-30 10:51:36 +0800 +0800'>March 30, 2025</span>&nbsp;·&nbsp;4 min</div></header><div class=post-content><p>[This review is intended solely for my personal learning]</p><p>Paper Info</p><blockquote><p>arXiv: 2405.03280v2<br>Title: Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural Vision from Slow Brain Activity<br>Authors: Yizhuo Lu, Changde Du, Chong Wang, Xuanliu Zhu, Liuyun Jiang, Xujin Li, and Huiguang He</p></blockquote><h2 id=prior-knowledge>Prior Knowledge<a hidden class=anchor aria-hidden=true href=#prior-knowledge>#</a></h2><ul><li><strong>Brain decoding with fMRI</strong>: Functional MRI measures brain activity through BOLD (Blood-Oxygen-Level Dependent) signals, which reflect neural activation across 3D voxel grids. Decoding methods aim to map these signals back to perceptual experiences like images or videos.</li><li><strong>Contrastive learning and CLIP</strong>: Contrastive loss functions (e.g., InfoNCE) train models to align representations from different modalities. CLIP is a pre-trained vision-language model that embeds text and images into a shared space, useful for representing high-level semantics.</li><li><strong>VQ-VAE (Vector Quantized Variational Autoencoder)</strong>: A generative model that discretizes image data into latent tokens, enabling compression and reconstruction. It’s commonly used in diffusion models for reducing the input dimensionality while preserving structure.</li><li><strong>Transformer and sparse causal attention</strong>: Transformers model long-range dependencies via attention. Sparse causal attention masks future tokens and restricts attention to a sparse set, ensuring temporal consistency while reducing computational cost—critical for decoding motion from fMRI.</li><li><strong>Stable Diffusion and U-Net</strong>: Stable Diffusion is a text-to-image model that generates visuals via iterative denoising in a latent space. Its U-Net backbone processes image latents at multiple resolutions and is commonly inflated for video generation.</li></ul><h2 id=goal>Goal<a hidden class=anchor aria-hidden=true href=#goal>#</a></h2><p>To accurately reconstruct dynamic natural vision (videos) from fMRI recordings by explicitly disentangling and decoding <strong>semantic</strong>, <strong>structural</strong>, and <strong>motion</strong> features — each aligned with distinct neural processes — in order to improve both reconstruction fidelity and neurobiological interpretability.</p><h2 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h2><p>The proposed system, <strong>Mind-Animator</strong>, adopts a <strong>two-stage pipeline</strong>:</p><h3 id=stage-1-fmri-to-feature-decoding>Stage 1: fMRI-to-Feature Decoding<a hidden class=anchor aria-hidden=true href=#stage-1-fmri-to-feature-decoding>#</a></h3><p>This stage maps the brain signal $x \in \mathbb{R}^n$ (fMRI voxels) into three disentangled representations:</p><ol><li><p><strong>Semantic Decoder</strong></p><ul><li>Projects fMRI into the CLIP (ViT-B/32) latent space using a tri-modal contrastive learning setup:</li></ul><p>$$
L_{\mathrm{semantic}} = \alpha \cdot L_{\mathrm{InfoNCE}}(f, t) + (1 - \alpha) \cdot L_{\mathrm{InfoNCE}}(f, v)
$$</p><p>where $f$, $t$, $v$ are embeddings from fMRI, text, and video, respectively.</p><ul><li>A 3-layer MLP then maps $f$ to the text-conditioning format $c$ used by Stable Diffusion.</li></ul></li><li><p><strong>Structure Decoder</strong></p><ul><li>Uses VQ-VAE encoding of the <strong>first frame</strong> to represent low-level visual structure.</li><li>A 2-layer MLP is trained with MSE loss:</li></ul><p>$$
L_{\mathrm{structure}} = \frac{1}{B} \sum_{i=1}^{B} \left| D_{\mathrm{structure}}(f_i) - \Phi(v_{i,1}) \right|_2^2
$$</p></li><li><p><strong>Motion Decoder: Consistency Motion Generator (CMG)</strong></p><ul><li>A Transformer with <strong>sparse causal attention</strong> decodes inter-frame motion from latent video tokens.</li><li>Frame embeddings are predicted autoregressively using fMRI as conditioning input: $L_{\mathrm{motion}}$ = mean squared error between decoded and ground truth motion tokens</li><li>Cross-attention ensures that spatial details in fMRI directly affect motion prediction.</li></ul></li></ol><h3 id=stage-2-feature-to-video-generation>Stage 2: Feature-to-Video Generation<a hidden class=anchor aria-hidden=true href=#stage-2-feature-to-video-generation>#</a></h3><ul><li>Instead of training a video generation model, the authors <strong>inflate</strong> a text-to-image Stable Diffusion model into a per-frame generator.</li><li>Reconstructed features are passed frame-by-frame into the U-Net (SD) and then decoded by VQ-VAE to reconstruct the video clip.</li></ul><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><p>Evaluated on <strong>CC2017</strong>, <strong>HCP</strong>, and <strong>Algonauts2021</strong> datasets.</p><h3 id=quantitative-results-cc2017>Quantitative Results (CC2017)<a hidden class=anchor aria-hidden=true href=#quantitative-results-cc2017>#</a></h3><table><thead><tr><th>Metric</th><th>Mind-Animator</th><th>Best Baseline (Mind-Video)</th></tr></thead><tbody><tr><td>SSIM</td><td><strong>0.321</strong></td><td>0.177</td></tr><tr><td>PSNR</td><td>9.22</td><td>8.87</td></tr><tr><td>Hue-PCC</td><td>0.786</td><td>0.768</td></tr><tr><td>CLIP-PCC</td><td>0.425</td><td>0.409</td></tr><tr><td>EPE (↓)</td><td><strong>5.42</strong></td><td>6.12</td></tr><tr><td>VIFI</td><td><strong>0.608</strong></td><td>0.593</td></tr></tbody></table><ul><li><strong>Retrieval accuracy</strong> on CC2017 (Top-10, Large Set): 2.39% vs. 1.28% (Mind-Video).</li><li>Maintains robustness even on extended datasets, demonstrating generalization.</li></ul><h3 id=ablation-study>Ablation Study<a hidden class=anchor aria-hidden=true href=#ablation-study>#</a></h3><table><thead><tr><th>Model Variant</th><th>SSIM</th><th>VIFI</th><th>EPE ↓</th></tr></thead><tbody><tr><td>Full Model</td><td>0.319</td><td>0.604</td><td>5.57</td></tr><tr><td>w/o Semantic Decoder</td><td>0.097</td><td>0.523</td><td>8.72</td></tr><tr><td>w/o Structure Decoder</td><td>0.184</td><td>0.555</td><td>7.68</td></tr><tr><td>w/o Motion Decoder</td><td>0.136</td><td>0.585</td><td>6.37</td></tr></tbody></table><p>Removing any one of the decoders leads to a noticeable drop in quality, validating the model’s modular design.</p><h3 id=motion-validity>Motion Validity<a hidden class=anchor aria-hidden=true href=#motion-validity>#</a></h3><ul><li>A shuffle test confirms that <strong>motion information arises from fMRI</strong>, not model bias.</li><li>EPE is a more sensitive motion metric than CLIP-PCC in this setting.</li></ul><h3 id=interpretability>Interpretability<a hidden class=anchor aria-hidden=true href=#interpretability>#</a></h3><ul><li>Voxel-wise maps show:<ul><li><strong>High-level cortex (HVC)</strong> encodes semantics.</li><li><strong>V1 and MT</strong> jointly encode motion.</li><li><strong>Ventral cortex</strong> for structure.</li></ul></li></ul><p>This aligns well with neuroscience literature and supports the brain-inspired design of the decoders.</p><h2 id=limitations>Limitations<a hidden class=anchor aria-hidden=true href=#limitations>#</a></h2><ul><li>Low temporal resolution of fMRI limits fine-grained motion fidelity.</li><li>Static structure assumption (only the first frame is used).</li><li>Potential inductive bias from frozen pretrained components.</li></ul><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><ul><li>The paper: <a href=https://arxiv.org/abs/2405.03280v2>https://arxiv.org/abs/2405.03280v2</a></li><li>This note was written with the assistance of Generative AI and is based on the content and results presented in the original paper.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://HowardHsuuu.github.io/tags/brain-decoding/>Brain Decoding</a></li><li><a href=https://HowardHsuuu.github.io/tags/fmri/>FMRI</a></li><li><a href=https://HowardHsuuu.github.io/tags/video-reconstruction/>Video Reconstruction</a></li><li><a href=https://HowardHsuuu.github.io/tags/diffusion/>Diffusion</a></li><li><a href=https://HowardHsuuu.github.io/tags/iclr-2025/>ICLR 2025</a></li></ul><nav class=paginav><a class=next href=https://HowardHsuuu.github.io/posts/lr/actadd/><span class=title>»</span><br><span>[LR] Steering Language Models With Activation Engineering</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://HowardHsuuu.github.io/>Notes</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script type=text/javascript src=/js/canvas-nest.js count=80 color=102,255,178 opacity=1></script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>