<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[LR] Quixer: A Quantum Transformer Model | Notes</title>
<meta name=keywords content="Quantum Computing,Quantum Machine Learning,Transformer"><meta name=description content="[This review is intended solely for my personal learning]
[Qiix at Qiskit Hackathon Taiwan 2024 was inspired by this paper]
Paper Info

arXiv: 2406.04305
Title: Quixer: A Quantum Transformer Model
Authors: Nikhil Khatri, Gabriel Matos, Luuk Coopmans, Stephen Clark (Quantinuum)
Prior Knowledge


Linear Combination of Unitaries (LCU)
The LCU technique allows the combination of multiple unitary operators into one overall operator using additional control qubits and postselection. Given unitaries $U_0, U_1, \dots, U_{n-1}$ with complex coefficients $b_0, b_1, \dots, b_{n-1}$, one can effectively prepare a block-encoded operator
$$
M = \sum_{j=0}^{n-1} b_j U_j.
$$"><meta name=author content><link rel=canonical href=https://HowardHsuuu.github.io/posts/lr/quixer/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://HowardHsuuu.github.io/images/icon.png><link rel=icon type=image/png sizes=16x16 href=https://HowardHsuuu.github.io/images/icon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://HowardHsuuu.github.io/images/icon-32x32.png><link rel=apple-touch-icon href=https://HowardHsuuu.github.io/images/icon.png><link rel=mask-icon href=https://HowardHsuuu.github.io/images/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://HowardHsuuu.github.io/posts/lr/quixer/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-...some_hash... crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-...some_hash... crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-...some_hash... crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="[LR] Quixer: A Quantum Transformer Model"><meta property="og:description" content="[This review is intended solely for my personal learning]
[Qiix at Qiskit Hackathon Taiwan 2024 was inspired by this paper]
Paper Info

arXiv: 2406.04305
Title: Quixer: A Quantum Transformer Model
Authors: Nikhil Khatri, Gabriel Matos, Luuk Coopmans, Stephen Clark (Quantinuum)
Prior Knowledge


Linear Combination of Unitaries (LCU)
The LCU technique allows the combination of multiple unitary operators into one overall operator using additional control qubits and postselection. Given unitaries $U_0, U_1, \dots, U_{n-1}$ with complex coefficients $b_0, b_1, \dots, b_{n-1}$, one can effectively prepare a block-encoded operator
$$
M = \sum_{j=0}^{n-1} b_j U_j.
$$"><meta property="og:type" content="article"><meta property="og:url" content="https://HowardHsuuu.github.io/posts/lr/quixer/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-09-27T19:56:26+08:00"><meta property="article:modified_time" content="2024-09-27T19:56:26+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="[LR] Quixer: A Quantum Transformer Model"><meta name=twitter:description content="[This review is intended solely for my personal learning]
[Qiix at Qiskit Hackathon Taiwan 2024 was inspired by this paper]
Paper Info

arXiv: 2406.04305
Title: Quixer: A Quantum Transformer Model
Authors: Nikhil Khatri, Gabriel Matos, Luuk Coopmans, Stephen Clark (Quantinuum)
Prior Knowledge


Linear Combination of Unitaries (LCU)
The LCU technique allows the combination of multiple unitary operators into one overall operator using additional control qubits and postselection. Given unitaries $U_0, U_1, \dots, U_{n-1}$ with complex coefficients $b_0, b_1, \dots, b_{n-1}$, one can effectively prepare a block-encoded operator
$$
M = \sum_{j=0}^{n-1} b_j U_j.
$$"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Topics","item":"https://HowardHsuuu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Literature Review","item":"https://HowardHsuuu.github.io/posts/lr/"},{"@type":"ListItem","position":3,"name":"[LR] Quixer: A Quantum Transformer Model","item":"https://HowardHsuuu.github.io/posts/lr/quixer/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[LR] Quixer: A Quantum Transformer Model","name":"[LR] Quixer: A Quantum Transformer Model","description":"[This review is intended solely for my personal learning]\n[Qiix at Qiskit Hackathon Taiwan 2024 was inspired by this paper]\nPaper Info\narXiv: 2406.04305\nTitle: Quixer: A Quantum Transformer Model\nAuthors: Nikhil Khatri, Gabriel Matos, Luuk Coopmans, Stephen Clark (Quantinuum)\nPrior Knowledge Linear Combination of Unitaries (LCU)\nThe LCU technique allows the combination of multiple unitary operators into one overall operator using additional control qubits and postselection. Given unitaries $U_0, U_1, \\dots, U_{n-1}$ with complex coefficients $b_0, b_1, \\dots, b_{n-1}$, one can effectively prepare a block-encoded operator $$ M = \\sum_{j=0}^{n-1} b_j U_j. $$\n","keywords":["Quantum Computing","Quantum Machine Learning","Transformer"],"articleBody":"[This review is intended solely for my personal learning]\n[Qiix at Qiskit Hackathon Taiwan 2024 was inspired by this paper]\nPaper Info\narXiv: 2406.04305\nTitle: Quixer: A Quantum Transformer Model\nAuthors: Nikhil Khatri, Gabriel Matos, Luuk Coopmans, Stephen Clark (Quantinuum)\nPrior Knowledge Linear Combination of Unitaries (LCU)\nThe LCU technique allows the combination of multiple unitary operators into one overall operator using additional control qubits and postselection. Given unitaries $U_0, U_1, \\dots, U_{n-1}$ with complex coefficients $b_0, b_1, \\dots, b_{n-1}$, one can effectively prepare a block-encoded operator $$ M = \\sum_{j=0}^{n-1} b_j U_j. $$\nQuantum Singular Value Transformation (QSVT)\nQSVT is a framework that applies polynomial transformations to the singular values (or spectrum) of a block-encoded matrix. In practice, if $M$ is block-encoded, QSVT can implement a transformation $$ P(M) = c_d M^d + \\cdots + c_1 M + c_0 I, $$ where the coefficients ${c_i}$ are real numbers. This mechanism is essential in quantum algorithms that require nontrivial matrix functions.\nClassical Transformers\nModern language models use attention mechanisms (typically multi-head dot-product self-attention) to mix token information. Variants like FNet and linear-attention have shown that alternative mixing methods can achieve competitive performance.\nQuantum Neural Networks\nPrior work has explored quantum analogues of classical neural networks (e.g., quantum RNNs). However, constructing a full-scale quantum Transformer for NLP remains largely uncharted until the advent of models like Quixer.\nGoal To design and validate a quantum Transformer model—Quixer—that:\nReplaces classical self-attention with an LCU-based token mixing strategy. Introduces nonlinearity via a QSVT-based polynomial transformation of the combined operator. Demonstrates viable performance on the Penn Treebank (PTB) language modeling task, benchmarked against classical neural baselines. Method Unitary Token Embeddings\nEach token’s embedding $\\vec{w}$ is passed through a trainable matrix $W_E$, yielding angles $\\theta_{\\vec{w}}$. These angles configure a parameterized quantum circuit $U(\\theta_{\\vec{w}})$ on $q$ qubits, thereby encoding the token as a unitary operator.\nMixing via LCU\nFor a context of $n$ tokens, each token is associated with a unitary $U_j$. The model learns coefficients ${b_j}$ to form $ M = \\sum_{j=0}^{n-1} b_j U_j. $ An LCU circuit is used to prepare this operator via block encoding and postselection on an ancilla qubit.\nNonlinearity via QSVT\nA polynomial transformation is applied to $M$: $ P(M) = c_d M^d + \\cdots + c_1 M + c_0 I, $ where the coefficients ${c_i}$ are trained analogously to activation function parameters in classical networks. This is implemented by interleaving applications of the block-encoded $M$ with controlled phase gates on an ancilla, as prescribed by the QSVT framework.\nFeed-Forward and Output\nAfter applying $P(M)$ to the data register initialized in $|0\\rangle$, a trainable feed-forward unitary $U_{\\mathrm{FF}}$ is applied. Multiple Pauli observables (e.g., $X$, $Y$, $Z$ on each qubit) are measured to yield a classical vector $\\vec{o}$. Finally, a classical network $f_{\\text{out}}$ maps $\\vec{o}$ to the next-token probabilities.\nResource Estimates\nQubits: Requires $q$ qubits for data, $\\lceil \\log_2(n)\\rceil$ qubits for the LCU control register, plus additional ancillas for QSVT and postselection. Gate Complexity: Roughly on the order of $d,n,g,\\log(n)$ (or better), where $g$ denotes the gate count for each parameterized token unitary $U_j$. Results Penn Treebank Evaluation\nQuixer processes a context window of 32 tokens to predict the next token. In simulation, it achieves a test perplexity of approximately 122.0, which is competitive with certain LSTM and FNet baselines. Postselection success rates for the LCU and QSVT steps are on the order of a few percent, emphasizing the need for high-fidelity quantum hardware.\nComparison to Classical Models\nQuixer outperforms smaller LSTM variants and is on par with an FNet of similar dimensionality, though it lags behind a classical Transformer of comparable scale. Nonetheless, it marks the first demonstration of a quantum Transformer on a standard language corpus like PTB.\nConclusion Quixer leverages LCU-based mixing and QSVT-based nonlinearity to construct a quantum Transformer model. While the current results are based on classical simulation and does not yet outperform state-of-the-art classical models, the model provides a promising blueprint for future quantum NLP systems as hardware improves. Its modular design also allows for various specializations (e.g., fixed versus trainable polynomial coefficients), paving the way for a family of quantum Transformer architectures.\nLimitations Barren Plateaus\nLarge parametric quantum circuits risk encountering vanishing gradients as the number of qubits increases. Postselection Overhead\nThe reliance on successful postselection in the LCU and QSVT steps reduces the overall fraction of successful circuit runs. Hardware Constraints\nPresent-day quantum devices are limited in both qubit count and fidelity, which constrains the scale of models that can be implemented. Classical Simulation\nThe experiments are performed on classical simulators; performance under realistic quantum noise and error correction remains to be evaluated. Reference The paper: https://arxiv.org/abs/2406.04305 This note was written with the assistance of Generative AI and is based on the content and results presented in the original paper. ","wordCount":"796","inLanguage":"en-us","datePublished":"2024-09-27T19:56:26+08:00","dateModified":"2024-09-27T19:56:26+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://HowardHsuuu.github.io/posts/lr/quixer/"},"publisher":{"@type":"Organization","name":"Notes","logo":{"@type":"ImageObject","url":"https://HowardHsuuu.github.io/images/icon.png"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://HowardHsuuu.github.io/ accesskey=h title="Notes (Alt + H)"><img src=https://HowardHsuuu.github.io/images/icon.png alt aria-label=logo height=35>Notes</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://HowardHsuuu.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://HowardHsuuu.github.io/posts/ title=Topics><span>Topics</span></a></li><li><a href=https://HowardHsuuu.github.io/tags title=Tags><span>Tags</span></a></li><li><a href=https://HowardHsuuu.github.io/aboutme/ title="About Me"><span>About Me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://HowardHsuuu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://HowardHsuuu.github.io/posts/>Topics</a>&nbsp;»&nbsp;<a href=https://HowardHsuuu.github.io/posts/lr/>Literature Review</a></div><h1 class="post-title entry-hint-parent">[LR] Quixer: A Quantum Transformer Model</h1><div class=post-meta><span title='2024-09-27 19:56:26 +0800 +0800'>September 27, 2024</span>&nbsp;·&nbsp;4 min</div></header><div class=post-content><p>[This review is intended solely for my personal learning]</p><p>[<strong>Qiix</strong> at Qiskit Hackathon Taiwan 2024 was inspired by this paper]</p><p>Paper Info</p><blockquote><p>arXiv: 2406.04305<br>Title: Quixer: A Quantum Transformer Model<br>Authors: Nikhil Khatri, Gabriel Matos, Luuk Coopmans, Stephen Clark (Quantinuum)</p></blockquote><h2 id=prior-knowledge>Prior Knowledge<a hidden class=anchor aria-hidden=true href=#prior-knowledge>#</a></h2><ul><li><p><strong>Linear Combination of Unitaries (LCU)</strong><br>The LCU technique allows the combination of multiple unitary operators into one overall operator using additional control qubits and postselection. Given unitaries $U_0, U_1, \dots, U_{n-1}$ with complex coefficients $b_0, b_1, \dots, b_{n-1}$, one can effectively prepare a block-encoded operator
$$
M = \sum_{j=0}^{n-1} b_j U_j.
$$</p></li><li><p><strong>Quantum Singular Value Transformation (QSVT)</strong><br>QSVT is a framework that applies polynomial transformations to the singular values (or spectrum) of a block-encoded matrix. In practice, if $M$ is block-encoded, QSVT can implement a transformation
$$
P(M) = c_d M^d + \cdots + c_1 M + c_0 I,
$$
where the coefficients ${c_i}$ are real numbers. This mechanism is essential in quantum algorithms that require nontrivial matrix functions.</p></li><li><p><strong>Classical Transformers</strong><br>Modern language models use attention mechanisms (typically multi-head dot-product self-attention) to mix token information. Variants like FNet and linear-attention have shown that alternative mixing methods can achieve competitive performance.</p></li><li><p><strong>Quantum Neural Networks</strong><br>Prior work has explored quantum analogues of classical neural networks (e.g., quantum RNNs). However, constructing a full-scale quantum Transformer for NLP remains largely uncharted until the advent of models like Quixer.</p></li></ul><h2 id=goal>Goal<a hidden class=anchor aria-hidden=true href=#goal>#</a></h2><p>To design and validate a quantum Transformer model—<strong>Quixer</strong>—that:</p><ol><li>Replaces classical self-attention with an LCU-based token mixing strategy.</li><li>Introduces nonlinearity via a QSVT-based polynomial transformation of the combined operator.</li><li>Demonstrates viable performance on the Penn Treebank (PTB) language modeling task, benchmarked against classical neural baselines.</li></ol><h2 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h2><ol><li><p><strong>Unitary Token Embeddings</strong><br>Each token’s embedding $\vec{w}$ is passed through a trainable matrix $W_E$, yielding angles $\theta_{\vec{w}}$. These angles configure a parameterized quantum circuit $U(\theta_{\vec{w}})$ on $q$ qubits, thereby encoding the token as a unitary operator.</p></li><li><p><strong>Mixing via LCU</strong><br>For a context of $n$ tokens, each token is associated with a unitary $U_j$. The model learns coefficients ${b_j}$ to form
$
M = \sum_{j=0}^{n-1} b_j U_j.
$
An LCU circuit is used to prepare this operator via block encoding and postselection on an ancilla qubit.</p></li><li><p><strong>Nonlinearity via QSVT</strong><br>A polynomial transformation is applied to $M$:
$
P(M) = c_d M^d + \cdots + c_1 M + c_0 I,
$
where the coefficients ${c_i}$ are trained analogously to activation function parameters in classical networks. This is implemented by interleaving applications of the block-encoded $M$ with controlled phase gates on an ancilla, as prescribed by the QSVT framework.</p></li><li><p><strong>Feed-Forward and Output</strong><br>After applying $P(M)$ to the data register initialized in $|0\rangle$, a trainable feed-forward unitary $U_{\mathrm{FF}}$ is applied. Multiple Pauli observables (e.g., $X$, $Y$, $Z$ on each qubit) are measured to yield a classical vector $\vec{o}$. Finally, a classical network $f_{\text{out}}$ maps $\vec{o}$ to the next-token probabilities.</p></li><li><p><strong>Resource Estimates</strong></p><ul><li><strong>Qubits</strong>: Requires $q$ qubits for data, $\lceil \log_2(n)\rceil$ qubits for the LCU control register, plus additional ancillas for QSVT and postselection.</li><li><strong>Gate Complexity</strong>: Roughly on the order of $d,n,g,\log(n)$ (or better), where $g$ denotes the gate count for each parameterized token unitary $U_j$.</li></ul></li></ol><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><ul><li><p><strong>Penn Treebank Evaluation</strong><br>Quixer processes a context window of 32 tokens to predict the next token. In simulation, it achieves a test perplexity of approximately 122.0, which is competitive with certain LSTM and FNet baselines. Postselection success rates for the LCU and QSVT steps are on the order of a few percent, emphasizing the need for high-fidelity quantum hardware.</p></li><li><p><strong>Comparison to Classical Models</strong><br>Quixer outperforms smaller LSTM variants and is on par with an FNet of similar dimensionality, though it lags behind a classical Transformer of comparable scale. Nonetheless, it marks the first demonstration of a quantum Transformer on a standard language corpus like PTB.</p></li></ul><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>Quixer leverages LCU-based mixing and QSVT-based nonlinearity to construct a quantum Transformer model. While the current results are based on classical simulation and does not yet outperform state-of-the-art classical models, the model provides a promising blueprint for future quantum NLP systems as hardware improves. Its modular design also allows for various specializations (e.g., fixed versus trainable polynomial coefficients), paving the way for a family of quantum Transformer architectures.</p><h2 id=limitations>Limitations<a hidden class=anchor aria-hidden=true href=#limitations>#</a></h2><ol><li><strong>Barren Plateaus</strong><br>Large parametric quantum circuits risk encountering vanishing gradients as the number of qubits increases.</li><li><strong>Postselection Overhead</strong><br>The reliance on successful postselection in the LCU and QSVT steps reduces the overall fraction of successful circuit runs.</li><li><strong>Hardware Constraints</strong><br>Present-day quantum devices are limited in both qubit count and fidelity, which constrains the scale of models that can be implemented.</li><li><strong>Classical Simulation</strong><br>The experiments are performed on classical simulators; performance under realistic quantum noise and error correction remains to be evaluated.</li></ol><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><ul><li>The paper: <a href=https://arxiv.org/abs/2406.04305>https://arxiv.org/abs/2406.04305</a></li><li>This note was written with the assistance of Generative AI and is based on the content and results presented in the original paper.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://HowardHsuuu.github.io/tags/quantum-computing/>Quantum Computing</a></li><li><a href=https://HowardHsuuu.github.io/tags/quantum-machine-learning/>Quantum Machine Learning</a></li><li><a href=https://HowardHsuuu.github.io/tags/transformer/>Transformer</a></li></ul><nav class=paginav><a class=prev href=https://HowardHsuuu.github.io/posts/lr/clonemator/><span class=title>«</span><br><span>[LR] Clonemator: Spatiotemporal Clones in VR</span>
</a><a class=next href=https://HowardHsuuu.github.io/posts/lr/lucid-interface/><span class=title>»</span><br><span>[LR] Interfacing with Lucid Dreams</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://HowardHsuuu.github.io/>Notes</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script type=text/javascript src=/js/canvas-nest.js count=80 color=102,255,178 opacity=1></script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>