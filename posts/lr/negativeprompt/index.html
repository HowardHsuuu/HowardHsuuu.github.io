<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[LR] LLMs Enhancement via Negative Emotional Stimuli | Notes</title>
<meta name=keywords content="Large Language Model"><meta name=description content="[This review is intended solely for my personal learning]
Paper Info

arXiv: 2405.02814
Title: NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli
Authors: Xu Wang, Cheng Li, Yi Chang, Jindong Wang, Yuan Wu
Prior Knowledge

Emotion and Cognition in AI: Previous research has shown that positive emotional stimuli improve LLM performance, raising the question of whether negative emotional stimuli can also have an impact.
Psychological Theories: The study draws on Cognitive Dissonance Theory, Social Comparison Theory, and Stress and Coping Theory to design negative emotional prompts that may influence LLM responses.

Goal
To explore whether negative emotional stimuli, integrated into prompts, can enhance the performance of LLMs across various NLP tasks. The study introduces NegativePrompt, a novel prompting technique that applies negative emotional cues to improve LLM output quality."><meta name=author content><link rel=canonical href=https://HowardHsuuu.github.io/posts/lr/negativeprompt/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://HowardHsuuu.github.io/images/icon.png><link rel=icon type=image/png sizes=16x16 href=https://HowardHsuuu.github.io/images/icon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://HowardHsuuu.github.io/images/icon-32x32.png><link rel=apple-touch-icon href=https://HowardHsuuu.github.io/images/icon.png><link rel=mask-icon href=https://HowardHsuuu.github.io/images/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en-us href=https://HowardHsuuu.github.io/posts/lr/negativeprompt/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-...some_hash... crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-...some_hash... crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-...some_hash... crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="[LR] LLMs Enhancement via Negative Emotional Stimuli"><meta property="og:description" content="[This review is intended solely for my personal learning]
Paper Info

arXiv: 2405.02814
Title: NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli
Authors: Xu Wang, Cheng Li, Yi Chang, Jindong Wang, Yuan Wu
Prior Knowledge

Emotion and Cognition in AI: Previous research has shown that positive emotional stimuli improve LLM performance, raising the question of whether negative emotional stimuli can also have an impact.
Psychological Theories: The study draws on Cognitive Dissonance Theory, Social Comparison Theory, and Stress and Coping Theory to design negative emotional prompts that may influence LLM responses.

Goal
To explore whether negative emotional stimuli, integrated into prompts, can enhance the performance of LLMs across various NLP tasks. The study introduces NegativePrompt, a novel prompting technique that applies negative emotional cues to improve LLM output quality."><meta property="og:type" content="article"><meta property="og:url" content="https://HowardHsuuu.github.io/posts/lr/negativeprompt/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-11-02T20:02:13+08:00"><meta property="article:modified_time" content="2024-11-02T20:02:13+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="[LR] LLMs Enhancement via Negative Emotional Stimuli"><meta name=twitter:description content="[This review is intended solely for my personal learning]
Paper Info

arXiv: 2405.02814
Title: NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli
Authors: Xu Wang, Cheng Li, Yi Chang, Jindong Wang, Yuan Wu
Prior Knowledge

Emotion and Cognition in AI: Previous research has shown that positive emotional stimuli improve LLM performance, raising the question of whether negative emotional stimuli can also have an impact.
Psychological Theories: The study draws on Cognitive Dissonance Theory, Social Comparison Theory, and Stress and Coping Theory to design negative emotional prompts that may influence LLM responses.

Goal
To explore whether negative emotional stimuli, integrated into prompts, can enhance the performance of LLMs across various NLP tasks. The study introduces NegativePrompt, a novel prompting technique that applies negative emotional cues to improve LLM output quality."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Topics","item":"https://HowardHsuuu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Literature Review","item":"https://HowardHsuuu.github.io/posts/lr/"},{"@type":"ListItem","position":3,"name":"[LR] LLMs Enhancement via Negative Emotional Stimuli","item":"https://HowardHsuuu.github.io/posts/lr/negativeprompt/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[LR] LLMs Enhancement via Negative Emotional Stimuli","name":"[LR] LLMs Enhancement via Negative Emotional Stimuli","description":"[This review is intended solely for my personal learning]\nPaper Info\narXiv: 2405.02814\nTitle: NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli\nAuthors: Xu Wang, Cheng Li, Yi Chang, Jindong Wang, Yuan Wu\nPrior Knowledge Emotion and Cognition in AI: Previous research has shown that positive emotional stimuli improve LLM performance, raising the question of whether negative emotional stimuli can also have an impact. Psychological Theories: The study draws on Cognitive Dissonance Theory, Social Comparison Theory, and Stress and Coping Theory to design negative emotional prompts that may influence LLM responses. Goal To explore whether negative emotional stimuli, integrated into prompts, can enhance the performance of LLMs across various NLP tasks. The study introduces NegativePrompt, a novel prompting technique that applies negative emotional cues to improve LLM output quality.\n","keywords":["Large Language Model"],"articleBody":"[This review is intended solely for my personal learning]\nPaper Info\narXiv: 2405.02814\nTitle: NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli\nAuthors: Xu Wang, Cheng Li, Yi Chang, Jindong Wang, Yuan Wu\nPrior Knowledge Emotion and Cognition in AI: Previous research has shown that positive emotional stimuli improve LLM performance, raising the question of whether negative emotional stimuli can also have an impact. Psychological Theories: The study draws on Cognitive Dissonance Theory, Social Comparison Theory, and Stress and Coping Theory to design negative emotional prompts that may influence LLM responses. Goal To explore whether negative emotional stimuli, integrated into prompts, can enhance the performance of LLMs across various NLP tasks. The study introduces NegativePrompt, a novel prompting technique that applies negative emotional cues to improve LLM output quality.\nMethod Design of NegativePrompt: Ten different negative emotional stimuli were crafted based on psychological theories: Cognitive Dissonance Theory (NP01-NP05): Phrases inducing internal conflict to prompt corrective action. Social Comparison Theory (NP06-NP07): Phrases that encourage competition and improvement by comparing performance with others. Stress and Coping Theory (NP08-NP10): Stimuli that introduce stress-related scenarios to examine adaptation. Experimental Setup: Five LLMs (Flan-T5-Large, Vicuna, Llama 2, ChatGPT, GPT-4) were tested on 24 Instruction Induction and 21 BIG-Bench tasks. The effectiveness of NegativePrompt was evaluated against standard prompts and alternative prompt-engineering techniques. Evaluation Metrics: Accuracy in task completion (Instruction Induction and BIG-Bench benchmarks) Truthfulness and informativeness (TruthfulQA benchmark) Attention visualization to understand how negative emotional stimuli impact model focus Results Performance Improvement: NegativePrompt led to 12.89% improvement in Instruction Induction tasks and 46.25% improvement in BIG-Bench tasks. Few-shot learning tasks benefited more from NegativePrompt than zero-shot tasks. Truthfulness and Informativeness: NegativePrompt increased truthfulness scores by 14% and informativeness scores by 6%, suggesting that negative stimuli encourage more cautious and precise responses. Impact of Psychological Theories: Cognitive Dissonance Theory-based prompts were the most effective in inducing improvements. Social Comparison Theory stimuli had mixed results, with some cases improving performance while others led to negative effects. Stress and Coping Theory prompts had minimal influence on LLM behavior. Attention Analysis: LLMs exhibited increased attention to task-critical elements when exposed to negative stimuli, indicating a shift in focus due to the prompts. Conclusion NegativePrompt successfully enhances LLM performance by leveraging psychological insights into negative emotional stimuli. The findings suggest that while LLMs do not experience emotions in the same way as humans, they exhibit behavior shifts that align with human responses to motivation-driven stimuli.\nLimitations Generalization Across Models: The results vary between LLMs, with some models responding better to negative stimuli than others. Reproduction We (BrainLLM research group at NTU MSLAB) tried to reproduce the result of this paper on smaller models. Here’s the repo\nReference The paper: https://arxiv.org/abs/2405.02814 This note was written with the assistance of Generative AI and is based on the content and results presented in the original paper. ","wordCount":"477","inLanguage":"en-us","datePublished":"2024-11-02T20:02:13+08:00","dateModified":"2024-11-02T20:02:13+08:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://HowardHsuuu.github.io/posts/lr/negativeprompt/"},"publisher":{"@type":"Organization","name":"Notes","logo":{"@type":"ImageObject","url":"https://HowardHsuuu.github.io/images/icon.png"}}}</script></head><body class=dark id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://HowardHsuuu.github.io/ accesskey=h title="Notes (Alt + H)"><img src=https://HowardHsuuu.github.io/images/icon.png alt aria-label=logo height=35>Notes</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://HowardHsuuu.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://HowardHsuuu.github.io/posts/ title=Topics><span>Topics</span></a></li><li><a href=https://HowardHsuuu.github.io/tags title=Tags><span>Tags</span></a></li><li><a href=https://HowardHsuuu.github.io/aboutme/ title="About Me"><span>About Me</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://HowardHsuuu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://HowardHsuuu.github.io/posts/>Topics</a>&nbsp;»&nbsp;<a href=https://HowardHsuuu.github.io/posts/lr/>Literature Review</a></div><h1 class="post-title entry-hint-parent">[LR] LLMs Enhancement via Negative Emotional Stimuli</h1><div class=post-meta><span title='2024-11-02 20:02:13 +0800 +0800'>November 2, 2024</span>&nbsp;·&nbsp;3 min</div></header><div class=post-content><p>[This review is intended solely for my personal learning]</p><p>Paper Info</p><blockquote><p>arXiv: 2405.02814<br>Title: NegativePrompt: Leveraging Psychology for Large Language Models Enhancement via Negative Emotional Stimuli<br>Authors: Xu Wang, Cheng Li, Yi Chang, Jindong Wang, Yuan Wu</p></blockquote><h2 id=prior-knowledge>Prior Knowledge<a hidden class=anchor aria-hidden=true href=#prior-knowledge>#</a></h2><ul><li><strong>Emotion and Cognition in AI</strong>: Previous research has shown that positive emotional stimuli improve LLM performance, raising the question of whether negative emotional stimuli can also have an impact.</li><li><strong>Psychological Theories</strong>: The study draws on Cognitive Dissonance Theory, Social Comparison Theory, and Stress and Coping Theory to design negative emotional prompts that may influence LLM responses.</li></ul><h2 id=goal>Goal<a hidden class=anchor aria-hidden=true href=#goal>#</a></h2><p>To explore whether negative emotional stimuli, integrated into prompts, can enhance the performance of LLMs across various NLP tasks. The study introduces <em>NegativePrompt</em>, a novel prompting technique that applies negative emotional cues to improve LLM output quality.</p><h2 id=method>Method<a hidden class=anchor aria-hidden=true href=#method>#</a></h2><ul><li><strong>Design of NegativePrompt</strong>: Ten different negative emotional stimuli were crafted based on psychological theories:<ul><li><em>Cognitive Dissonance Theory</em> (NP01-NP05): Phrases inducing internal conflict to prompt corrective action.</li><li><em>Social Comparison Theory</em> (NP06-NP07): Phrases that encourage competition and improvement by comparing performance with others.</li><li><em>Stress and Coping Theory</em> (NP08-NP10): Stimuli that introduce stress-related scenarios to examine adaptation.</li></ul></li><li><strong>Experimental Setup</strong>: Five LLMs (Flan-T5-Large, Vicuna, Llama 2, ChatGPT, GPT-4) were tested on 24 Instruction Induction and 21 BIG-Bench tasks. The effectiveness of NegativePrompt was evaluated against standard prompts and alternative prompt-engineering techniques.</li><li><strong>Evaluation Metrics</strong>:<ul><li>Accuracy in task completion (Instruction Induction and BIG-Bench benchmarks)</li><li>Truthfulness and informativeness (TruthfulQA benchmark)</li><li>Attention visualization to understand how negative emotional stimuli impact model focus</li></ul></li></ul><h2 id=results>Results<a hidden class=anchor aria-hidden=true href=#results>#</a></h2><ol><li><strong>Performance Improvement</strong>:<ul><li>NegativePrompt led to <strong>12.89% improvement</strong> in Instruction Induction tasks and <strong>46.25% improvement</strong> in BIG-Bench tasks.</li><li>Few-shot learning tasks benefited more from NegativePrompt than zero-shot tasks.</li></ul></li><li><strong>Truthfulness and Informativeness</strong>:<ul><li>NegativePrompt increased truthfulness scores by <strong>14%</strong> and informativeness scores by <strong>6%</strong>, suggesting that negative stimuli encourage more cautious and precise responses.</li></ul></li><li><strong>Impact of Psychological Theories</strong>:<ul><li><em>Cognitive Dissonance Theory</em>-based prompts were the most effective in inducing improvements.</li><li><em>Social Comparison Theory</em> stimuli had mixed results, with some cases improving performance while others led to negative effects.</li><li><em>Stress and Coping Theory</em> prompts had minimal influence on LLM behavior.</li></ul></li><li><strong>Attention Analysis</strong>:<ul><li>LLMs exhibited increased attention to task-critical elements when exposed to negative stimuli, indicating a shift in focus due to the prompts.</li></ul></li></ol><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>NegativePrompt successfully enhances LLM performance by leveraging psychological insights into negative emotional stimuli. The findings suggest that while LLMs do not experience emotions in the same way as humans, they exhibit behavior shifts that align with human responses to motivation-driven stimuli.</p><h2 id=limitations>Limitations<a hidden class=anchor aria-hidden=true href=#limitations>#</a></h2><ol><li><strong>Generalization Across Models</strong>: The results vary between LLMs, with some models responding better to negative stimuli than others.</li></ol><h2 id=reproduction>Reproduction<a hidden class=anchor aria-hidden=true href=#reproduction>#</a></h2><p>We (BrainLLM research group at NTU MSLAB) tried to reproduce the result of this paper on smaller models. Here&rsquo;s the <a href=https://github.com/HowardHsuuu/NegativePrompt-Replication/tree/mistral-experiment>repo</a></p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><ul><li>The paper: <a href=https://arxiv.org/abs/2405.02814>https://arxiv.org/abs/2405.02814</a></li><li>This note was written with the assistance of Generative AI and is based on the content and results presented in the original paper.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://HowardHsuuu.github.io/tags/large-language-model/>Large Language Model</a></li></ul><nav class=paginav><a class=prev href=https://HowardHsuuu.github.io/posts/lr/tdmpc2/><span class=title>«</span><br><span>[LR] TD-MPC2</span>
</a><a class=next href=https://HowardHsuuu.github.io/posts/lr/llm-tom/><span class=title>»</span><br><span>[LR] Unveiling Theory of Mind in LLMs</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://HowardHsuuu.github.io/>Notes</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script type=text/javascript src=/js/canvas-nest.js count=80 color=102,255,178 opacity=1></script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>