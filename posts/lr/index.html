<!doctype html><html lang=en-us dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Literature Review | Notes</title>
<meta name=keywords content><meta name=description content="Literature Review - Notes"><meta name=author content><link rel=canonical href=https://HowardHsuuu.github.io/posts/lr/><link crossorigin=anonymous href=/assets/css/stylesheet.b609c58d5c11bb90b1a54e04005d74ad1ddf22165eb79f5533967e57df9c3b50.css integrity="sha256-tgnFjVwRu5CxpU4EAF10rR3fIhZet59VM5Z+V9+cO1A=" rel="preload stylesheet" as=style><link rel=icon href=https://HowardHsuuu.github.io/images/icon.png><link rel=icon type=image/png sizes=16x16 href=https://HowardHsuuu.github.io/images/icon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://HowardHsuuu.github.io/images/icon-32x32.png><link rel=apple-touch-icon href=https://HowardHsuuu.github.io/images/icon.png><link rel=mask-icon href=https://HowardHsuuu.github.io/images/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate type=application/rss+xml href=https://HowardHsuuu.github.io/posts/lr/index.xml><link rel=alternate hreflang=en-us href=https://HowardHsuuu.github.io/posts/lr/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.css integrity=sha384-...some_hash... crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/katex.min.js integrity=sha384-...some_hash... crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/auto-render.min.js integrity=sha384-...some_hash... crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"\\(",right:"\\)",display:!1},{left:"$",right:"$",display:!1}],throwOnError:!1})})</script><meta property="og:title" content="Literature Review"><meta property="og:description" content><meta property="og:type" content="website"><meta property="og:url" content="https://HowardHsuuu.github.io/posts/lr/"><meta name=twitter:card content="summary"><meta name=twitter:title content="Literature Review"><meta name=twitter:description content><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Topics","item":"https://HowardHsuuu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Literature Review","item":"https://HowardHsuuu.github.io/posts/lr/"}]}</script></head><body class="list dark" id=top><script>localStorage.getItem("pref-theme")==="light"&&document.body.classList.remove("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://HowardHsuuu.github.io/ accesskey=h title="Notes (Alt + H)"><img src=https://HowardHsuuu.github.io/images/icon.png alt aria-label=logo height=35>Notes</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://HowardHsuuu.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://HowardHsuuu.github.io/posts/ title=Topics><span>Topics</span></a></li><li><a href=https://HowardHsuuu.github.io/tags title=Tags><span>Tags</span></a></li><li><a href=https://HowardHsuuu.github.io/aboutme/ title="About Me"><span>About Me</span></a></li></ul></nav></header><main class=main><header class=page-header><div class=breadcrumbs><a href=https://HowardHsuuu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://HowardHsuuu.github.io/posts/>Topics</a></div><h1>Literature Review</h1></header><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[LR] Reconstruction of Dynamic Natural Vision from Slow Brain Activity</h2></header><div class=entry-content><p>[This review is intended solely for my personal learning]
Paper Info
arXiv: 2405.03280v2
Title: Animate Your Thoughts: Decoupled Reconstruction of Dynamic Natural Vision from Slow Brain Activity
Authors: Yizhuo Lu, Changde Du, Chong Wang, Xuanliu Zhu, Liuyun Jiang, Xujin Li, and Huiguang He
Prior Knowledge Brain decoding with fMRI: Functional MRI measures brain activity through BOLD (Blood-Oxygen-Level Dependent) signals, which reflect neural activation across 3D voxel grids. Decoding methods aim to map these signals back to perceptual experiences like images or videos. Contrastive learning and CLIP: Contrastive loss functions (e.g., InfoNCE) train models to align representations from different modalities. CLIP is a pre-trained vision-language model that embeds text and images into a shared space, useful for representing high-level semantics. VQ-VAE (Vector Quantized Variational Autoencoder): A generative model that discretizes image data into latent tokens, enabling compression and reconstruction. It’s commonly used in diffusion models for reducing the input dimensionality while preserving structure. Transformer and sparse causal attention: Transformers model long-range dependencies via attention. Sparse causal attention masks future tokens and restricts attention to a sparse set, ensuring temporal consistency while reducing computational cost—critical for decoding motion from fMRI. Stable Diffusion and U-Net: Stable Diffusion is a text-to-image model that generates visuals via iterative denoising in a latent space. Its U-Net backbone processes image latents at multiple resolutions and is commonly inflated for video generation. Goal To accurately reconstruct dynamic natural vision (videos) from fMRI recordings by explicitly disentangling and decoding semantic, structural, and motion features — each aligned with distinct neural processes — in order to improve both reconstruction fidelity and neurobiological interpretability.
...</p></div><footer class=entry-footer><span title='2025-03-30 10:51:36 +0800 +0800'>March 30, 2025</span>&nbsp;·&nbsp;4 min</footer><a class=entry-link aria-label="post link to [LR] Reconstruction of Dynamic Natural Vision from Slow Brain Activity" href=https://HowardHsuuu.github.io/posts/lr/mind-animator/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[LR] Steering Language Models With Activation Engineering</h2></header><div class=entry-content><p>[This review is intended solely for my personal learning]
Paper Info
arXiv: 2308.10248
Title: Steering Language Models With Activation Engineering
Authors: Alexander Matt Turner, David Udell, Nantas Nardelli, Sam Ringer, Tom McGrath, Eric Michaud, Mantas Mazeika
Prior Knowledge Steering Language Models: Traditionally achieved through techniques such as prompt engineering, fine-tuning, and reinforcement learning from human feedback (RLHF), which often require substantial computational resources and specialized datasets. Internal Activation Manipulation: Exploring internal activations of neural networks can offer fine-grained control over their outputs without the cost associated with retraining or extensive model tuning. Goal The paper aims to introduce and validate a lightweight inference-time technique, termed Activation Addition (ActAdd), for steering the output of large language models (LLMs). This method targets latent capabilities of LLMs, such as controlled sentiment expression or reduced output toxicity, without the need for additional training.
...</p></div><footer class=entry-footer><span title='2025-03-26 20:12:03 +0800 +0800'>March 26, 2025</span>&nbsp;·&nbsp;2 min</footer><a class=entry-link aria-label="post link to [LR] Steering Language Models With Activation Engineering" href=https://HowardHsuuu.github.io/posts/lr/actadd/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[LR] Representation Engineering: Top-Down AI Transparency</h2></header><div class=entry-content><p>[This review is intended solely for my personal learning]
Paper Info
arXiv: 2310.01405v4
Title: Representation Engineering: A Top-Down Approach to AI Transparency
Authors: Evan Hubinger, Ajeya Cotra, Buck Shlegeris, Tom Lieberum, Nicholas Joseph, Owain Evans, Nicholas Schiefer, Oliver Zhang, Jan Brauner, Collin Burns, Leo Gao, Ryan Greenblatt
Prior Knowledge Interpretability in AI: Traditional approaches often focus on low-level features such as neurons or circuits. However, such bottom-up analysis struggles with high-level abstractions like honesty or power-seeking. Representation Learning: Neural networks form internal embeddings of concepts during training, enabling powerful generalization and emergent behavior. Mechanistic Interpretability: Aims to reverse-engineer model internals to understand reasoning processes, but often lacks tools for directly modifying internal representations to improve safety. Transparency and Alignment: Transparency is critical for ensuring model behavior is aligned with human intentions and values, particularly to avoid deceptive alignment and inner misalignment. Goal The paper proposes and develops Representation Engineering (RepE), a new top-down framework for interpretability and model control. Rather than focusing on neurons or circuits, RepE centers on representations of high-level cognitive concepts and aims to understand, detect, and manipulate them. The core goal is to advance transparency methods that are directly useful for improving AI safety, including controlling undesirable behaviors like deception or power-seeking.
...</p></div><footer class=entry-footer><span title='2025-03-25 20:12:03 +0800 +0800'>March 25, 2025</span>&nbsp;·&nbsp;3 min</footer><a class=entry-link aria-label="post link to [LR] Representation Engineering: Top-Down AI Transparency" href=https://HowardHsuuu.github.io/posts/lr/repe/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[LR] Swarm Intelligence for EEG Channel Selection in Emotion Recognition</h2></header><div class=entry-content><p>[This review is intended solely for my personal learning]
Paper Info
DOI: 10.1007/978-3-031-05409-9_23
Title: A Swarm Intelligence Approach: Combination of Different EEG-Channel Optimization Techniques to Enhance Emotion Recognition
Authors: Sabahudin Balic, Lukas Kleybolte, and Christian Märtin
Prior Knowledge EEG-based Emotion Recognition: EEG captures electrical brain activity via multiple electrodes. Its high temporal resolution makes it suitable for decoding emotional states in real-time. However, the full 32-channel setup is often computationally expensive and redundant. Swarm Intelligence: Algorithms like Particle Swarm Optimization (PSO), Cuckoo Search (CS), and Grey Wolf Optimizer (GWO) mimic social behavior of animals to solve optimization problems. Feature vs. Channel Selection: Traditional feature selection targets discriminative features across frequency bands, while channel selection focuses on spatially optimizing electrode positions. Goal To evaluate and compare different EEG channel selection techniques—both classical and swarm intelligence-based—to optimize emotion classification performance while significantly reducing computation time.
...</p></div><footer class=entry-footer><span title='2025-03-22 19:21:17 +0800 +0800'>March 22, 2025</span>&nbsp;·&nbsp;3 min</footer><a class=entry-link aria-label="post link to [LR] Swarm Intelligence for EEG Channel Selection in Emotion Recognition" href=https://HowardHsuuu.github.io/posts/lr/eeg-emotion-recognition/></a></article><article class=post-entry><header class=entry-header><h2 class=entry-hint-parent>[LR] Predicting Whole-Brain Neural Dynamics from Prefrontal Cortex fNIRS Signal</h2></header><div class=entry-content><p>[This review is intended solely for my personal learning]
Paper Info
DOI: 10.1101/2024.11.17.623979
Title: Predicting whole-brain neural dynamics from prefrontal cortex fNIRS signal during movie-watching
Authors: Shan Gao, Ryleigh Nash, Shannon Burns, Yuan Chang Leong
Prior Knowledge Functional near-infrared spectroscopy (fNIRS) offers a more accessible alternative to functional magnetic resonance imaging (fMRI) but is limited by shallow cortical penetration. Prior research has demonstrated potential in predicting deep-brain fMRI signals from fNIRS data through linear predictive models trained on simultaneous fMRI and fNIRS measurements.
...</p></div><footer class=entry-footer><span title='2025-03-17 15:32:01 +0800 +0800'>March 17, 2025</span>&nbsp;·&nbsp;3 min</footer><a class=entry-link aria-label="post link to [LR] Predicting Whole-Brain Neural Dynamics from Prefrontal Cortex fNIRS Signal" href=https://HowardHsuuu.github.io/posts/lr/fnirs2fmri/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://HowardHsuuu.github.io/posts/lr/page/2/>&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://HowardHsuuu.github.io/>Notes</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script type=text/javascript src=/js/canvas-nest.js count=80 color=102,255,178 opacity=1></script><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>